"""
Script to generate embeddings from pre-trained model for PPI prediction.
This script processes training, validation, and test data to generate embeddings for binder and wt sequences.
"""
import torch
import pandas as pd
import argparse
import os
import sys
from pathlib import Path
from tqdm import tqdm

# Add paths to sys.path for imports
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(current_file.parent.parent / "inference"))

# Import from downstream_tasks
from downstream_tasks.tasks.ppi_prediction.load_data import (
    load_ppi_data,
    prepare_sequences_and_labels_for_embedding_generation
)
from downstream_tasks.utils.inference.embedding_generator_inference import EmbeddingGeneratorInference


def infer_model_type(pretrained_model_name: str) -> str:
    """
    æ ¹æ®é¢„è®­ç»ƒæ¨¡å‹åç§°æ¨æ–­æ¨¡å‹ç±»å‹ã€‚

    @param pretrained_model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
    @return: æ¨¡å‹ç±»å‹ ('esm2' æˆ– 'esmc')
    """
    return EmbeddingGeneratorInference.infer_model_type(pretrained_model_name)


def main():
    parser = argparse.ArgumentParser(description="Generate embeddings for PPI prediction")
    parser.add_argument(
        "--pretrained_model_name",
        type=str,
        default=None,
        help="Pretrained model name from HuggingFace. If None, uses default ESM2 model."
    )
    parser.add_argument(
        "--layer_index",
        type=int,
        default=None,
        help="Layer index to extract (1-based for esmc, 0-based for esm2). If None, uses last layer (default: None)"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=32,
        help="Batch size for inference"
    )
    parser.add_argument(
        "--max_sequence_length",
        type=int,
        default=None,
        help="Maximum sequence length for a single window. "
             "If None, uses default from model config (typically 512). "
             "For sequences longer than this, sliding window will be used."
    )
    parser.add_argument(
        "--use_sliding_window",
        action="store_true",
        default=False,
        help="Use sliding window for sequences longer than max_sequence_length."
    )
    parser.add_argument(
        "--window_overlap",
        type=float,
        default=0.3,
        help="Overlap ratio between sliding windows (0.0 to 1.0). "
             "Default 0.3 means 30%% overlap."
    )
    parser.add_argument(
        "--train_ratio",
        type=float,
        default=0.7,
        help="Ratio for training set (default: 0.7)"
    )
    parser.add_argument(
        "--valid_ratio",
        type=float,
        default=0.15,
        help="Ratio for validation set (default: 0.15)"
    )
    parser.add_argument(
        "--test_ratio",
        type=float,
        default=0.15,
        help="Ratio for test set (default: 0.15)"
    )
    parser.add_argument(
        "--random_seed",
        type=int,
        default=42,
        help="Random seed for data splitting (default: 42)"
    )
    
    args = parser.parse_args()

    # Set default pretrained model name
    if args.pretrained_model_name is None:
        args.pretrained_model_name = "facebook/esm2_t30_150M_UR50D"

    # Infer model type from pretrained model name
    model_type = infer_model_type(args.pretrained_model_name)
    print(f"ğŸ” Inferred model type: {model_type} from {args.pretrained_model_name}")

    # Create model-specific directory structure
    model_short_name = args.pretrained_model_name.replace('/', '_').replace('facebook_', '').replace('esm2_', 'esm2-').replace('esm_', 'esm-')
    layer_suffix = f"layer{args.layer_index}" if args.layer_index is not None else "last"
    model_layer_dir = f"{model_short_name}_{layer_suffix}"

    # Create task-specific subdirectory
    output_dir = os.path.join(os.getcwd(), "embeddings")
    task_dir = os.path.join(output_dir, model_layer_dir, "ppi")
    os.makedirs(task_dir, exist_ok=True)
    print(f"ğŸ“ Embeddings will be stored in: {task_dir}")

    # Fixed data path for PPI
    data_path = "/home/zz/zheng/ptm-mlm/downstream_tasks/tasks/ppi_prediction/PTM experimental evidence.csv"

    # Load data
    print("ğŸ“– Loading PPI data...")
    train_df, valid_df, test_df = load_ppi_data(
        data_path,
        train_ratio=args.train_ratio,
        valid_ratio=args.valid_ratio,
        test_ratio=args.test_ratio,
        random_seed=args.random_seed
    )
    
    # ğŸ”§ PPIä»»åŠ¡ç‰¹æ®Šå¤„ç†ï¼š
    # - binderå’Œwtï¼šä½¿ç”¨final layerï¼ˆæœ€åä¸€å±‚ï¼‰ç”Ÿæˆembeddingsï¼ˆoriginalï¼‰
    # - PTMï¼šä½¿ç”¨WTåºåˆ—ï¼ˆoriginalï¼‰ï¼Œä½†ç”¨specific layerï¼ˆlayer_indexå‚æ•°æŒ‡å®šçš„å±‚ï¼‰ç”Ÿæˆembeddingsï¼Œç•™ç»™blockå¤„ç†
    
    # Initialize EmbeddingGeneratorInference for original sequences (final layer)
    print(f"\nğŸš€ Initializing {model_type.upper()} embedding generator for original sequences (final layer)...")
    original_inferencer = EmbeddingGeneratorInference(
        model_type=model_type,
        model_name=args.pretrained_model_name,
        layer_index=None,  # Noneè¡¨ç¤ºä½¿ç”¨æœ€åä¸€å±‚ï¼ˆfinal layerï¼‰
        max_sequence_length=args.max_sequence_length
    )
    
    # Initialize EmbeddingGeneratorInference for PTM sequences (specific layer)
    # PTMä½¿ç”¨WTåºåˆ—ï¼Œä½†ç”¨specific layerç”Ÿæˆembeddings
    if args.layer_index is not None:
        print(f"\nğŸš€ Initializing {model_type.upper()} embedding generator for PTM sequences (layer {args.layer_index})...")
        ptm_inferencer = EmbeddingGeneratorInference(
            model_type=model_type,
            model_name=args.pretrained_model_name,
            layer_index=args.layer_index,  # ä½¿ç”¨specific layer
            max_sequence_length=args.max_sequence_length
        )
    else:
        # å¦‚æœlayer_indexä¸ºNoneï¼ŒPTMä¹Ÿä½¿ç”¨final layer
        print(f"\nğŸš€ PTM will use final layer (same as original)...")
        ptm_inferencer = original_inferencer
    
    # Process each split
    for split_name, df in [("train", train_df), ("valid", valid_df), ("test", test_df)]:
        print("\n" + "="*70)
        print(f"ğŸ”„ Processing {split_name} data...")
        print("="*70)
        
        # å‡†å¤‡åºåˆ—å’Œæ ‡ç­¾ï¼ˆåªç”ŸæˆåŸå§‹åºåˆ—ï¼Œä¸ç”ŸæˆPTMåºåˆ—ï¼‰
        binder_sequences, wt_sequences, labels = prepare_sequences_and_labels_for_embedding_generation(df)
        
        if len(binder_sequences) == 0:
            print(f"âš ï¸  No valid samples in {split_name} set, skipping...")
            continue
        
        print(f"ğŸ“Š Processing {len(binder_sequences)} samples...")
        print(f"â„¹ï¸  Binder and WT embeddings: final layer (original)")
        print(f"â„¹ï¸  PTM embeddings: layer {args.layer_index if args.layer_index is not None else 'final'} (using WT sequences)")
        
        # ğŸ”§ ä¼˜åŒ–ï¼šæ ¹æ®layer_indexå†³å®šç”Ÿæˆç­–ç•¥
        if args.layer_index is None:
            # layer_indexä¸ºNoneï¼šæ‰€æœ‰embeddingséƒ½ä½¿ç”¨final layerï¼Œåˆ†åˆ«ç”Ÿæˆ
            print(f"\nğŸ”¹ Generating binder embeddings (final layer)...")
            binder_embeddings_tensor, binder_metadata_list, _ = original_inferencer.generate_batch_embeddings(
                binder_sequences,
                batch_size=args.batch_size,
                max_sequence_length=args.max_sequence_length if args.max_sequence_length else 512,
                use_sliding_window=args.use_sliding_window,
                window_overlap=args.window_overlap,
                layer_indices=[None]  # Noneè¡¨ç¤ºfinal layerï¼Œä½¿ç”¨æ–°æ¥å£
            )
            
            print(f"\nğŸ”¹ Generating WT embeddings (final layer, will be reused for PTM)...")
            wt_embeddings_tensor, wt_metadata_list, _ = original_inferencer.generate_batch_embeddings(
                wt_sequences,
                batch_size=args.batch_size,
                max_sequence_length=args.max_sequence_length if args.max_sequence_length else 512,
                use_sliding_window=args.use_sliding_window,
                window_overlap=args.window_overlap,
                layer_indices=[None]  # Noneè¡¨ç¤ºfinal layerï¼Œä½¿ç”¨æ–°æ¥å£
            )
            # PTMä½¿ç”¨ç›¸åŒçš„embeddingså’Œmetadata
            ptm_embeddings_tensor = wt_embeddings_tensor
            ptm_metadata_list = wt_metadata_list
            print(f"   âœ… PTM embeddings reused from WT (same layer)")
        else:
            # layer_indexä¸æ˜¯Noneï¼šä¸€æ¬¡æ€§ç”Ÿæˆfinal layerå’Œspecific layerçš„embeddings
            print(f"\nğŸ”¹ Generating embeddings (multiple layers in one pass)...")
            print(f"   - Binder: final layer")
            print(f"   - WT: final layer")
            print(f"   - PTM: layer {args.layer_index}")
            
            # ä¸€æ¬¡æ€§ç”Ÿæˆbinderçš„final layer embeddings
            binder_embeddings_tensor, binder_metadata_list, _ = original_inferencer.generate_batch_embeddings(
                binder_sequences,
                batch_size=args.batch_size,
                max_sequence_length=args.max_sequence_length if args.max_sequence_length else 512,
                use_sliding_window=args.use_sliding_window,
                window_overlap=args.window_overlap,
                layer_indices=[None]  # Noneè¡¨ç¤ºfinal layer
            )
            
            # ä¸€æ¬¡æ€§ç”ŸæˆWTçš„final layerå’ŒPTMçš„specific layer embeddings
            layer_indices = [None, args.layer_index]  # Noneè¡¨ç¤ºfinal layer
            result_dict = original_inferencer.generate_batch_embeddings(
                wt_sequences,
                batch_size=args.batch_size,
                max_sequence_length=args.max_sequence_length if args.max_sequence_length else 512,
                use_sliding_window=args.use_sliding_window,
                window_overlap=args.window_overlap,
                layer_indices=layer_indices  # ä¸€æ¬¡æ€§ç”Ÿæˆä¸¤å±‚
            )
            
            # æå–WT embeddings (final layer, layer_index=None)
            wt_embeddings_tensor, wt_metadata_list, _ = result_dict[None]
            
            # æå–PTM embeddings (specific layer)
            ptm_embeddings_tensor, ptm_metadata_list, _ = result_dict[args.layer_index]
        
        # Verify consistency
        binder_num_seqs = max(meta['sequence_id'] for meta in binder_metadata_list) + 1
        wt_num_seqs = max(meta['sequence_id'] for meta in wt_metadata_list) + 1
        ptm_num_seqs = max(meta['sequence_id'] for meta in ptm_metadata_list) + 1
        
        assert binder_num_seqs == wt_num_seqs == ptm_num_seqs == len(labels), \
            f"Mismatch in {split_name}: binder={binder_num_seqs}, wt={wt_num_seqs}, " \
            f"ptm={ptm_num_seqs}, labels={len(labels)}"
        
        # Save batch embeddings and metadata
        print(f"\nğŸ’¾ Saving {split_name} embeddings...")

        binder_emb_path = os.path.join(task_dir, f"{split_name}_binder_embeddings.pt")
        binder_metadata_path = os.path.join(task_dir, f"{split_name}_binder_embeddings_metadata.json")
        wt_emb_path = os.path.join(task_dir, f"{split_name}_wt_embeddings.pt")
        wt_metadata_path = os.path.join(task_dir, f"{split_name}_wt_embeddings_metadata.json")
        ptm_emb_path = os.path.join(task_dir, f"{split_name}_ptm_embeddings.pt")
        ptm_metadata_path = os.path.join(task_dir, f"{split_name}_ptm_embeddings_metadata.json")
        labels_path = os.path.join(task_dir, f"{split_name}_labels.pt")
        
        torch.save(binder_embeddings_tensor, binder_emb_path)
        EmbeddingGeneratorInference.save_metadata(binder_metadata_list, binder_metadata_path)
        torch.save(wt_embeddings_tensor, wt_emb_path)
        EmbeddingGeneratorInference.save_metadata(wt_metadata_list, wt_metadata_path)
        torch.save(ptm_embeddings_tensor, ptm_emb_path)
        EmbeddingGeneratorInference.save_metadata(ptm_metadata_list, ptm_metadata_path)
        torch.save(labels, labels_path)
        
        print(f"âœ… Saved {split_name} data: {len(labels)} samples")
        print(f"   - Binder embeddings (final layer): {len(binder_sequences)} sequences")
        print(f"   - WT embeddings (final layer): {len(wt_sequences)} sequences")
        print(f"   - PTM embeddings (layer {args.layer_index if args.layer_index is not None else 'final'}, using WT sequences): {len(wt_sequences)} sequences")
    
    print("\n" + "="*70)
    print("ğŸ‰ Embedding generation completed!")
    print("="*70)


if __name__ == "__main__":
    main()

