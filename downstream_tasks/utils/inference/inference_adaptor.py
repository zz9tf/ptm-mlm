"""
Adaptor æ¨ç†è„šæœ¬ï¼Œç”¨äºæ¥å—embeddingsè¾“å…¥å¹¶ç”Ÿæˆé€‚é…åçš„embeddingsã€‚
æ­¤è„šæœ¬åŠ è½½é€‚é…å™¨æ¨¡å‹ï¼ˆLoRAç­‰ï¼‰å¹¶å¯¹è¾“å…¥çš„embeddingsè¿›è¡Œå¤„ç†ã€‚

è¿™æ˜¯ä¸€ä¸ªå…±äº«æ¨¡å—ï¼Œç”¨äºæ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡ã€‚
"""
import torch
from tqdm import tqdm

from main_pipeline.models.model import PTMModel

class AdaptorInference:
    """
    LoRA æ¨¡å‹æ¨ç†ç±»ï¼Œç”¨äºä»é¢„è®­ç»ƒçš„ LoRA checkpoint ç”Ÿæˆ block è¾“å‡ºã€‚
    ä»…è¿”å› block çš„è¾“å‡ºç»“æœï¼Œä¸ç»è¿‡ headsã€‚
    å¿…é¡»ä½¿ç”¨ ESM C 600M æ¨¡å‹ã€‚
    """
    
    def __init__(self, checkpoint_path: str, device: str = None, embed_dim: int = 1152):
        """
        åˆå§‹åŒ–é€‚é…å™¨æ¨ç†æ¨¡å‹ã€‚

        @param checkpoint_path: è®­ç»ƒå¥½çš„é€‚é…å™¨æ¨¡å‹ checkpoint è·¯å¾„ï¼ˆ.ckpt æ–‡ä»¶ï¼‰
        @param device: è¿è¡Œæ¨ç†çš„è®¾å¤‡ï¼ˆNone è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼‰
        @param embed_dim: è¾“å…¥embeddingsçš„ç»´åº¦ï¼ˆé»˜è®¤1152ï¼Œå¯¹åº”ESM-C 600Mï¼‰
        """
        # ç¡®å®šè®¾å¤‡
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # Load adaptor checkpoint
        print(f"ğŸ“¦ æ­£åœ¨ä» {checkpoint_path} åŠ è½½é€‚é…å™¨æ¨¡å‹...")
        ckpt = torch.load(checkpoint_path, map_location='cpu')
        model_state_dict = ckpt["model"]
        model_config_dict = ckpt["config"]

        # ä»é…ç½®ä¸­è·å–æ¨¡å‹å‚æ•°
        embed_dim = model_config_dict.get("embed_dim", embed_dim)  # ä½¿ç”¨å‚æ•°ä¸­çš„embed_dimä½œä¸ºé»˜è®¤å€¼
        vocab_size = model_config_dict.get("vocab_size", 32)  # é€‚é…å™¨ä¸éœ€è¦vocab
        d_model = model_config_dict.get("d_model", 512)
        block_config = model_config_dict.get("block_config", {"type": "lora"})

        # åˆå§‹åŒ– PTMModelï¼ˆåªä½¿ç”¨ blockï¼Œheads ä¸ä¼šè¢«ä½¿ç”¨ï¼‰
        self.model = PTMModel(
            embed_dim=embed_dim,
            vocab_size=vocab_size,
            d_model=d_model,
            block_config=block_config,
            heads_config=[],  # ä¸åˆ›å»º headsï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€è¦ block è¾“å‡º
            device=self.device,
        )

        # åŠ è½½æ¨¡å‹çŠ¶æ€
        msg = self.model.load_state_dict(model_state_dict, strict=False)
        print(f"ğŸ“ æ¨¡å‹åŠ è½½ä¿¡æ¯: {msg}")

        # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        self.model = self.model.to(self.device)
        self.model.eval()

        # ä»æ¨¡å‹é…ç½®è·å–éšè—å±‚å¤§å°
        self.hidden_size = d_model
        self.embed_dim = embed_dim

        print(f"âœ… é€‚é…å™¨æ¨¡å‹åŠ è½½æˆåŠŸï¼è¾“å…¥ç»´åº¦: {embed_dim}, è¾“å‡ºç»´åº¦: {d_model}")
        print(f"ğŸ”§ æ¨¡å¼: Adaptor Block (æ¥å—embeddingsè¾“å…¥ï¼Œç”Ÿæˆé€‚é…åè¾“å‡º)")
    
    
    @torch.no_grad()
    def process_embeddings(self, embeddings_list: list, return_pooled: bool = False):
        """
        å¤„ç†è¾“å…¥çš„embeddingsåˆ—è¡¨ï¼Œé€šè¿‡é€‚é…å™¨ç”Ÿæˆæ–°çš„embeddingsã€‚

        @param embeddings_list: embeddingså¼ é‡åˆ—è¡¨ï¼Œæ¯ä¸ªå½¢çŠ¶ä¸º (seq_len, embed_dim)
        @param return_pooled: å¦‚æœä¸º Trueï¼Œè¿”å›æ± åŒ–çš„ embeddingsï¼ˆå¹³å‡æ± åŒ–ï¼‰ã€‚
                            å¦‚æœä¸º Falseï¼Œè¿”å›åºåˆ—çº§åˆ«çš„ embeddingsï¼ˆæ‰€æœ‰ tokenï¼‰
        @returns: å¦‚æœ return_pooled=Trueï¼Œè¿”å›å½¢çŠ¶ä¸º (num_sequences, hidden_size) çš„å¼ é‡ï¼Œ
                 å¦‚æœ return_pooled=Falseï¼Œè¿”å›embeddingsåˆ—è¡¨ï¼Œæ¯ä¸ªå½¢çŠ¶ä¸º (seq_len, hidden_size)
        """
        all_outputs = []

        for embeddings in tqdm(embeddings_list, desc="å¤„ç†é€‚é…å™¨ embeddings"):
            # ç¡®ä¿embeddingsåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if isinstance(embeddings, torch.Tensor):
                if embeddings.device != self.device:
                    embeddings = embeddings.to(self.device)
            else:
                embeddings = torch.tensor(embeddings, device=self.device)

            # æ·»åŠ batchç»´åº¦: (seq_len, embed_dim) -> (1, seq_len, embed_dim)
            if embeddings.dim() == 2:
                embeddings = embeddings.unsqueeze(0)

            # é€šè¿‡é€‚é…å™¨blockå¤„ç†
            adapted_embeddings = self.model.block(embeddings)  # (1, seq_len, hidden_size)

            # ç§»é™¤batchç»´åº¦: (1, seq_len, hidden_size) -> (seq_len, hidden_size)
            adapted_embeddings = adapted_embeddings.squeeze(0)

            if return_pooled:
                # å¹³å‡æ± åŒ–æ•´ä¸ªåºåˆ—
                pooled = adapted_embeddings.mean(dim=0)  # (hidden_size,)
                all_outputs.append(pooled.cpu())
            else:
                # è¿”å›é€ä½ç½®embeddings
                all_outputs.append(adapted_embeddings.cpu())

        if return_pooled:
            # è¿”å›å¼ é‡: (num_sequences, hidden_size)
            outputs = torch.stack(all_outputs, dim=0)
        else:
            # è¿”å›åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º (seq_len, hidden_size)
            outputs = all_outputs

        return outputs
    


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ä»é¢„è®­ç»ƒ LoRA æ¨¡å‹ç”Ÿæˆ block è¾“å‡º")
    parser.add_argument("--checkpoint", type=str, required=True, help="æ¨¡å‹ checkpoint è·¯å¾„")
    parser.add_argument("--sequences", type=str, nargs="+", help="è¾“å…¥åºåˆ—")
    parser.add_argument("--output", type=str, help="ä¿å­˜è¾“å‡ºçš„è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=32, help="æ¨ç†æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--return_pooled", action="store_true", help="è¿”å›æ± åŒ–çš„è¾“å‡º")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨ç†æ¨¡å‹
    inferencer = AdaptorInference(args.checkpoint)
    
    # ç”Ÿæˆ block è¾“å‡º
    outputs = inferencer.generate_block_outputs(
        args.sequences,
        batch_size=args.batch_size,
        return_pooled=args.return_pooled
    )
    
    # å¦‚æœæä¾›äº†è¾“å‡ºè·¯å¾„ï¼Œä¿å­˜è¾“å‡º
    if args.output:
        torch.save(outputs, args.output)
        print(f"âœ… è¾“å‡ºå·²ä¿å­˜åˆ° {args.output}")
    else:
        print(f"ğŸ“Š ç”Ÿæˆçš„è¾“å‡ºå½¢çŠ¶: {outputs.shape}")

