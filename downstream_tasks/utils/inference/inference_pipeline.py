"""
Inference pipeline for downstream tasks.
This module provides utilities to load pre-computed embeddings, process them through adaptor checkpoints,
and prepare them for head training.
"""
import os
import sys
import torch
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any
from tqdm import tqdm

# Add project root to sys.path for importing main_pipeline
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.join(current_dir, '..', '..', '..')
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from .inference_adaptor import AdaptorInference
from .embedding_generator_inference import EmbeddingGeneratorInference
from collections import defaultdict
import json


class InferencePipeline:
    """
    æ¨ç†æµæ°´çº¿ç±»ï¼Œç”¨äºå¤„ç†ä»embeddingsåˆ°adaptorå†åˆ°headçš„å®Œæ•´æµç¨‹ã€‚
    """

    def __init__(self, embeddings_base_dir: str = "/home/zz/zheng/ptm-mlm/downstream_tasks/embeddings",
                 checkpoints_base_dir: str = "/home/zz/zheng/ptm-mlm/downstream_tasks/checkpoints"):
        """
        åˆå§‹åŒ–æ¨ç†æµæ°´çº¿ã€‚

        @param embeddings_base_dir: embeddingsåŸºç¡€ç›®å½•
        @param checkpoints_base_dir: checkpointsåŸºç¡€ç›®å½•
        """
        self.embeddings_base_dir = Path(embeddings_base_dir)
        self.checkpoints_base_dir = Path(checkpoints_base_dir)

    def get_embeddings_path(self, model_name: str, layer_index: int) -> Path:
        """
        æ ¹æ®model_nameå’Œlayer_indexç”Ÿæˆembeddingsè·¯å¾„ã€‚

        @param model_name: æ¨¡å‹åç§°
        @param layer_index: å±‚ç´¢å¼•
        @return: embeddingsç›®å½•è·¯å¾„
        """
        # æ„å»ºembeddingsç›®å½•åç§°
        embeddings_dir_name = f"{model_name}_layer{layer_index}"
        embeddings_path = self.embeddings_base_dir / embeddings_dir_name
        return embeddings_path

    def find_checkpoint_path(self, checkpoint_name: str) -> Path:
        """
        æ ¹æ®checkpointåç§°æ‰¾åˆ°checkpointæ–‡ä»¶è·¯å¾„ã€‚

        @param checkpoint_name: checkpointæ–‡ä»¶åï¼ˆä¸å«.ckptæ‰©å±•åï¼‰
        @return: checkpointæ–‡ä»¶å®Œæ•´è·¯å¾„
        """
        checkpoint_path = self.checkpoints_base_dir / f"{checkpoint_name}.ckpt"
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
        return checkpoint_path

    def load_embeddings_for_task(self, model_name: str, layer_index: int, task_name: str) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        ä¸ºç‰¹å®šä»»åŠ¡åŠ è½½embeddingsã€‚

        @param model_name: æ¨¡å‹åç§°
        @param layer_index: å±‚ç´¢å¼•
        @param task_name: ä»»åŠ¡åç§° ('nhas', 'p_site', 'ppi')
        @return: (embeddingså­—å…¸, metadataå­—å…¸) å…ƒç»„ï¼Œembeddingså­—å…¸åŒ…å«è®­ç»ƒ/éªŒè¯/æµ‹è¯•embeddingså’Œlabels
        """
        embeddings_path = self.get_embeddings_path(model_name, layer_index)
        task_path = embeddings_path / task_name

        if not task_path.exists():
            raise FileNotFoundError(f"Task embeddings not found: {task_path}")

        print(f"\nğŸ“¦ å¼€å§‹åŠ è½½ä»»åŠ¡æ•°æ®: {task_name}")
        print(f"   ğŸ“ ä»»åŠ¡è·¯å¾„: {task_path}")

        embeddings = {}
        metadata_dict = {}

        # åŒºåˆ† embeddings æ–‡ä»¶å’Œ labels æ–‡ä»¶
        # embeddings æ–‡ä»¶éœ€è¦ metadataï¼Œlabels æ–‡ä»¶ä¸éœ€è¦
        embedding_files = {
            'train': ['train_embeddings.pt', 'train_labels.pt'],
            'valid': ['valid_embeddings.pt', 'valid_labels.pt'],
            'test': ['test_embeddings.pt', 'test_labels.pt']
        }

        # å¯¹äºppiä»»åŠ¡ï¼Œè¿˜æœ‰é¢å¤–çš„embeddings
        # ğŸ”§ PPIä»»åŠ¡ç‰¹æ®Šå¤„ç†ï¼šåŠ è½½binderã€wtå’Œptmçš„embeddings
        # PTM embeddingså·²ç»ç”Ÿæˆï¼Œç›´æ¥loadï¼Œç„¶åé€šè¿‡adaptor blockå¤„ç†
        if task_name == 'ppi':
            embedding_files['train'].extend(['train_binder_embeddings.pt', 'train_wt_embeddings.pt', 'train_ptm_embeddings.pt'])
            embedding_files['valid'].extend(['valid_binder_embeddings.pt', 'valid_wt_embeddings.pt', 'valid_ptm_embeddings.pt'])
            embedding_files['test'].extend(['test_binder_embeddings.pt', 'test_wt_embeddings.pt', 'test_ptm_embeddings.pt'])

        # åˆ¤æ–­æ˜¯å¦ä¸º labels æ–‡ä»¶ï¼ˆä¸éœ€è¦ metadataï¼‰
        def is_label_file(filename: str) -> bool:
            """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦ä¸º labels æ–‡ä»¶"""
            return 'labels' in filename

        # è®¡ç®—æ€»æ–‡ä»¶æ•°ï¼ˆç”¨äºè¿›åº¦æ¡ï¼‰
        total_files = sum(len(files) for files in embedding_files.values())
        
        print(f"\nğŸ” æ‰«ææ–‡ä»¶å¹¶åŠ è½½æ•°æ®...")
        print(f"   é¢„è®¡åŠ è½½ {total_files} ä¸ªæ–‡ä»¶\n")
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºåŠ è½½è¿›åº¦
        with tqdm(total=total_files, desc="ğŸ“¦ åŠ è½½æ•°æ®æ–‡ä»¶", unit="file", 
                  bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]") as pbar:
            
            for split, files in embedding_files.items():
                pbar.set_postfix({"æ•°æ®é›†": split})
                
                for file in files:
                    file_path = task_path / file
                    pbar.set_postfix({"æ•°æ®é›†": split, "æ–‡ä»¶": file[:25] + "..." if len(file) > 25 else file})
                    
                    if file_path.exists():
                        key = file.replace('.pt', '')
                        
                        # åŠ è½½æ•°æ®æ–‡ä»¶
                        try:
                            embeddings[key] = torch.load(file_path, weights_only=False)
                            
                            # æ˜¾ç¤ºåŠ è½½çš„æ•°æ®å½¢çŠ¶/å¤§å°ï¼ˆé€šè¿‡ postfixï¼‰
                            if isinstance(embeddings[key], torch.Tensor):
                                shape_str = str(list(embeddings[key].shape))
                                pbar.set_postfix({
                                    "æ–‡ä»¶": file[:20] + "..." if len(file) > 20 else file,
                                    "å½¢çŠ¶": shape_str[:30]
                                })
                            elif isinstance(embeddings[key], list):
                                pbar.set_postfix({
                                    "æ–‡ä»¶": file[:20] + "..." if len(file) > 20 else file,
                                    "é•¿åº¦": len(embeddings[key])
                                })
                        except Exception as e:
                            pbar.close()
                            raise RuntimeError(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                        
                        # åªå¯¹ embeddings æ–‡ä»¶ï¼ˆé labelsï¼‰åŠ è½½ metadata
                        if not is_label_file(file):
                            metadata_file = file_path.parent / f"{file_path.stem}_metadata.json"
                            
                            if not metadata_file.exists():
                                pbar.close()
                                raise FileNotFoundError(
                                    f"âŒ Metadataæ–‡ä»¶æœªæ‰¾åˆ°: {metadata_file}\n"
                                    f"   Embeddingsæ–‡ä»¶éœ€è¦å¯¹åº”çš„metadataæ–‡ä»¶æ¥å¤„ç†windowså’Œç‰¹æ®Štokenã€‚\n"
                                    f"   è¯·å…ˆè¿è¡Œinferenceç”Ÿæˆembeddingså’Œmetadataã€‚"
                                )
                            
                            try:
                                metadata_dict[key] = EmbeddingGeneratorInference.load_metadata(str(metadata_file))
                                metadata_count = len(metadata_dict[key]) if isinstance(metadata_dict[key], list) else 1
                                pbar.set_postfix({
                                    "æ–‡ä»¶": file[:20] + "..." if len(file) > 20 else file,
                                    "metadata": f"{metadata_count}æ¡"
                                })
                            except Exception as e:
                                pbar.close()
                                raise RuntimeError(
                                    f"âŒ åŠ è½½metadataå¤±è´¥ {metadata_file}: {e}"
                                )
                    else:
                        # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¹Ÿæ›´æ–°è¿›åº¦æ¡
                        pbar.set_postfix({"æ–‡ä»¶": file[:20] + "..." if len(file) > 20 else file, "çŠ¶æ€": "âš ï¸ä¸å­˜åœ¨"})
                    
                    pbar.update(1)

        print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ!")
        print(f"   ğŸ“Š å·²åŠ è½½ {len(embeddings)} ä¸ªæ•°æ®æ–‡ä»¶")
        print(f"   ğŸ“‹ å·²åŠ è½½ {len(metadata_dict)} ä¸ªmetadataæ–‡ä»¶")

        # éªŒè¯ï¼šç¡®ä¿è‡³å°‘åŠ è½½äº†è®­ç»ƒæ•°æ®
        if task_name == 'ppi':
            # ğŸ”§ PPI ä»»åŠ¡éœ€è¦ binderã€wt å’Œ ptm embeddingsï¼ˆPTM embeddingså·²ç»ç”Ÿæˆï¼‰
            required_train_keys = ['train_binder_embeddings', 'train_wt_embeddings', 'train_ptm_embeddings']
            missing_keys = [key for key in required_train_keys if key not in embeddings]
            if missing_keys:
                raise FileNotFoundError(
                    f"âŒ æœªæ‰¾åˆ° PPI è®­ç»ƒæ•°æ®æ–‡ä»¶: {', '.join(missing_keys)}\n"
                    f"   ä»»åŠ¡è·¯å¾„: {task_path}\n"
                    f"   è¯·å…ˆè¿è¡Œ embedding generation è„šæœ¬ç”Ÿæˆæ•°æ®æ–‡ä»¶ã€‚\n"
                    f"   å¯¹äº PPI ä»»åŠ¡ï¼Œè¯·è¿è¡Œ: python utils/embeddings_generator/ppi_generator.py"
                )
        else:
            # å…¶ä»–ä»»åŠ¡éœ€è¦ train_embeddings
            if 'train_embeddings' not in embeddings:
                raise FileNotFoundError(
                    f"âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶ (train_embeddings.pt)\n"
                    f"   ä»»åŠ¡è·¯å¾„: {task_path}\n"
                    f"   è¯·å…ˆè¿è¡Œ embedding generation è„šæœ¬ç”Ÿæˆæ•°æ®æ–‡ä»¶ã€‚\n"
                    f"   å¯¹äº NHA ä»»åŠ¡ï¼Œè¯·è¿è¡Œ: python utils/embeddings_generator/nhas_generator.py"
                )

        return embeddings, metadata_dict

    def process_embeddings(self, embeddings: torch.Tensor,
                          metadata_list: List[Dict],
                          adaptor_checkpoint: Optional[str] = None,
                          device: str = "cuda",
                          batch_size: int = 32) -> List[torch.Tensor]:
        """
        å¤„ç† embeddingsï¼Œæ ¹æ®æ˜¯å¦æœ‰ checkpoint é€‰æ‹©å¤„ç†è·¯å¾„ã€‚
        
        - å¦‚æœæœ‰ checkpointï¼šé€šè¿‡ adaptor æ‰¹é‡å¤„ç† batch embeddingsï¼Œç„¶å merge
        - å¦‚æœæ²¡æœ‰ checkpointï¼šç›´æ¥å¤„ç†ï¼ˆç§»é™¤ç‰¹æ®Š tokenã€merge windowsï¼‰
        
        @param embeddings: è¾“å…¥embeddingsï¼Œå•ä¸ªå¤§çš„ batch tensor (total_items, max_seq_len, embed_dim)
        @param metadata_list: metadata åˆ—è¡¨ï¼ˆå¿…éœ€ï¼‰
        @param adaptor_checkpoint: adaptor checkpointåç§°ã€‚å¦‚æœä¸º Noneï¼Œç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ embeddings
        @param device: è®¾å¤‡
        @param batch_size: Batch size for adaptor processingï¼ˆå¦‚æœæœ‰ checkpointï¼‰
        @return: å¤„ç†åçš„embeddingsåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º (seq_len, hidden_size)
        """
        if embeddings.dim() != 3:
            raise ValueError(f"Expected 3D tensor (total_items, max_seq_len, embed_dim), got {embeddings.dim()}D")

        # éªŒè¯ batch_size å‚æ•°
        if batch_size <= 0:
            raise ValueError(f"batch_size must be positive, got {batch_size}")

        if adaptor_checkpoint is not None:
            # æœ‰ checkpointï¼šé€šè¿‡ adaptor æ‰¹é‡å¤„ç†
            checkpoint_path = self.find_checkpoint_path(adaptor_checkpoint)
            adaptor = AdaptorInference(str(checkpoint_path), device=device)
            
            # ğŸ”„ æŒ‰ batch_size åˆ†æ‰¹å¤„ç† embeddings
            total_items = embeddings.shape[0]
            processed_batches = []
            
            # è®¡ç®—æ€»æ‰¹æ¬¡æ•°ï¼ˆç”¨äºè¿›åº¦æ¡ï¼‰
            num_batches = (total_items + batch_size - 1) // batch_size
            
            with tqdm(total=num_batches, desc="ğŸ”„ Processing batches", unit="batch") as pbar:
                for batch_idx in range(0, total_items, batch_size):
                    # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç»“æŸç´¢å¼•
                    end_idx = min(batch_idx + batch_size, total_items)
                    
                    # æå–å½“å‰æ‰¹æ¬¡çš„ embeddings
                    batch_embeddings = embeddings[batch_idx:end_idx]
                    
                    # ğŸ” æ‰¾åˆ°å½“å‰æ‰¹æ¬¡å¯¹åº”çš„ metadataï¼ˆæ ¹æ® embedding_idxï¼‰
                    batch_metadata = []
                    for meta in metadata_list:
                        embedding_idx = meta.get('embedding_idx')
                        if embedding_idx is not None and batch_idx <= embedding_idx < end_idx:
                            # åˆ›å»ºæ–°çš„ metadata å‰¯æœ¬ï¼Œæ›´æ–° embedding_idx ä¸ºæ‰¹æ¬¡å†…çš„ç›¸å¯¹ç´¢å¼•
                            batch_meta = meta.copy()
                            batch_meta['embedding_idx'] = embedding_idx - batch_idx
                            batch_metadata.append(batch_meta)
                    
                    # éªŒè¯ï¼šç¡®ä¿æ‰¹æ¬¡æœ‰å¯¹åº”çš„ metadataï¼ˆè‡³å°‘åº”è¯¥æœ‰ä¸€ä¸ªï¼‰
                    if len(batch_metadata) == 0:
                        raise ValueError(
                            f"No metadata found for batch [{batch_idx}:{end_idx}]. "
                            f"This might indicate a mismatch between embeddings and metadata."
                        )
                    
                    # å¤„ç†å½“å‰æ‰¹æ¬¡
                    batch_processed_tensor, _ = adaptor.process_embeddings(
                        batch_embeddings,
                        metadata_list=batch_metadata
                    )
                    
                    processed_batches.append(batch_processed_tensor)
                    
                    pbar.set_postfix({
                        "æ‰¹æ¬¡": f"{batch_idx // batch_size + 1}/{num_batches}",
                        "items": f"{end_idx - batch_idx}/{total_items}"
                    })
                    pbar.update(1)
            
            # ğŸ”— åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„å¤„ç†ç»“æœ
            processed_batch = torch.cat(processed_batches, dim=0)
        else:
            # æ²¡æœ‰ checkpointï¼šç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ embeddingsï¼ˆç§»åŠ¨åˆ° CPUï¼‰
            processed_batch = embeddings.cpu()

        # ç»Ÿä¸€è¿›è¡Œ mergeï¼ˆå¦‚æœæœ‰ windowsï¼‰å’Œç§»é™¤ç‰¹æ®Š token
        final_embeddings = self._merge_embeddings(processed_batch, metadata_list)

        # è¿”å›listæ ¼å¼ï¼Œä¿æŒå˜é•¿åºåˆ—çš„çµæ´»æ€§
        return final_embeddings
    
    def _merge_embeddings(self, processed_embeddings_tensor: torch.Tensor, metadata_list: list) -> List[torch.Tensor]:
        """
        æ ¹æ® metadata åˆå¹¶ embeddingsï¼ˆå¦‚æœæœ‰ windowsï¼‰ï¼Œå¹¶ç§»é™¤ç‰¹æ®Š tokenã€‚
        
        @param processed_embeddings_tensor: å¤„ç†åçš„ batch tensor (total_items, max_seq_len, hidden_size)
        @param metadata_list: Metadata åˆ—è¡¨
        @return: åˆå¹¶åçš„ embeddings åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º (seq_len, hidden_size)
        """
        # æŒ‰ sequence_id åˆ†ç»„
        sequence_groups = defaultdict(list)
        
        # éå† metadata_listï¼Œæ ¹æ® embedding_idx ä»å¤§çš„ batch tensor ä¸­æå–å¯¹åº”çš„ embeddings
        for meta in metadata_list:
            seq_id = meta['sequence_id']
            embedding_idx = meta.get('embedding_idx')
            valid_length = meta.get('valid_length')
            
            if embedding_idx is None or embedding_idx >= processed_embeddings_tensor.shape[0]:
                raise ValueError(
                    f"Invalid embedding_idx {embedding_idx} for sequence {seq_id}. "
                    f"Total embeddings: {processed_embeddings_tensor.shape[0]}"
                )
            
            # ä»å¤§çš„ batch tensor ä¸­æå–å¯¹åº”çš„ embedding
            emb = processed_embeddings_tensor[embedding_idx, :valid_length, :]
            sequence_groups[seq_id].append((emb, meta))
        
        # å¯¹æ¯ä¸ªåºåˆ—è¿›è¡Œ mergeï¼ˆå¦‚æœéœ€è¦ï¼‰
        all_outputs = []
        num_sequences = max(meta['sequence_id'] for meta in metadata_list) + 1
        
        # è®¡ç®—éœ€è¦ merge çš„åºåˆ—æ•°é‡ï¼ˆæœ‰å¤šä¸ª windows çš„åºåˆ—ï¼‰
        sequences_to_merge = sum(1 for group in sequence_groups.values() if len(group) > 1)
        if sequences_to_merge > 0:
            merge_desc = f"Merging windows ({sequences_to_merge} sequences)"
        else:
            merge_desc = "Preparing final embeddings"
        
        with tqdm(total=num_sequences, desc=merge_desc, unit="seq") as pbar:
            for seq_id in range(num_sequences):
                if seq_id not in sequence_groups:
                    # å¦‚æœæ²¡æœ‰è¯¥åºåˆ—çš„æ•°æ®ï¼ŒæŠ›å‡ºé”™è¯¯ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
                    raise ValueError(
                        f"Missing sequence_id {seq_id} in embeddings. "
                        f"Expected {num_sequences} sequences, but sequence_id {seq_id} is missing. "
                        f"Available sequence_ids: {sorted(sequence_groups.keys())}"
                    )
                
                group = sequence_groups[seq_id]
                
                if len(group) == 1:
                    # å•ä¸ªå®Œæ•´åºåˆ—ï¼Œä¸éœ€è¦ merge
                    emb, meta = group[0]
                    # ç§»é™¤ç‰¹æ®Š tokenï¼ˆESM2: <cls> å’Œ <eos>ï¼ŒESM-C: BOS å’Œ EOSï¼‰
                    emb = self._remove_special_tokens(emb)
                    all_outputs.append(emb)
                else:
                    # å¤šä¸ª windowsï¼Œéœ€è¦ merge
                    windows_data = [(emb, meta['start_idx'], meta['end_idx']) for emb, meta in group]
                    merge_info_list = [meta.get('merge_info') for _, meta in group]  # æå– merge_info
                    seq_len = group[0][1]['seq_len']  # ä» metadata è·å–åºåˆ—é•¿åº¦ï¼ˆä¸åŒ…å«ç‰¹æ®Š tokenï¼‰
                    
                    # æ¨æ–­ hidden_size
                    hidden_size = group[0][0].shape[-1]
                    
                    merged_embeddings = self._merge_window_embeddings(
                        windows_data, seq_len, hidden_size, merge_info_list
                    )
                    all_outputs.append(merged_embeddings)
                
                pbar.update(1)
        
        return all_outputs
    
    def _merge_window_embeddings(self, windows_data: list, full_length: int, hidden_size: int, 
                                 merge_info_list: list = None):
        """
        Merge embeddings from multiple sliding windows.
        ä½¿ç”¨å¹³å‡æ± åŒ–åˆå¹¶é‡å åŒºåŸŸã€‚
        
        æ³¨æ„ï¼šwindow_emb åŒ…å«ç‰¹æ®Š tokenï¼ˆBOS/EOS æˆ– <cls>/<eos>ï¼‰ï¼Œ
        start_idx å’Œ end_idx æ˜¯ token ä½ç½®ï¼ˆç›¸å¯¹äºåŸå§‹åºåˆ—çš„ token ç´¢å¼•ï¼Œä¸åŒ…å«ç‰¹æ®Š tokenï¼‰ã€‚
        æ‰€ä»¥éœ€è¦å…ˆç§»é™¤ç‰¹æ®Š tokenï¼Œç„¶åå† mergeã€‚
        
        @param windows_data: List of (embeddings_tensor, start_idx, end_idx) tuples
                            embeddings_tensor åŒ…å«ç‰¹æ®Š token
                            start_idx å’Œ end_idx æ˜¯ token ä½ç½®ï¼ˆtoken ç´¢å¼•ï¼‰
        @param full_length: Full sequence length in tokensï¼ˆä¸åŒ…å«ç‰¹æ®Š tokenï¼‰
        @param hidden_size: Hidden dimension size
        @param merge_info_list: Optional list of merge_info dicts from metadataï¼Œç”¨äºè®°å½• merge ä¿¡æ¯
        @returns: Merged embeddings tensor of shape (full_length, hidden_size) on CPUï¼ˆå·²ç§»é™¤ç‰¹æ®Š tokenï¼‰
        """
        # åœ¨ CPU ä¸Š mergeï¼ˆå› ä¸º processed_embeddings å·²ç»åœ¨ CPU ä¸Šï¼‰
        merged_embeddings = torch.zeros(full_length, hidden_size)
        count_tensor = torch.zeros(full_length)  # è®°å½•æ¯ä¸ªä½ç½®è¢«å¤šå°‘ä¸ª windows è¦†ç›–
        
        for idx, (window_emb, start_idx, end_idx) in enumerate(windows_data):
            # window_emb å·²ç»åœ¨ CPU ä¸Šï¼ˆä» process_embeddings è¿”å›ï¼‰
            # å…ˆç§»é™¤ç‰¹æ®Š tokenï¼ˆBOS/EOS æˆ– <cls>/<eos>ï¼‰
            window_emb = self._remove_special_tokens(window_emb)
            
            window_len = window_emb.shape[0]
            expected_window_len = end_idx - start_idx
            
            # ç¡®ä¿ window embedding é•¿åº¦åŒ¹é…æœŸæœ›çš„çª—å£é•¿åº¦
            if window_len != expected_window_len:
                # å¦‚æœ embedding æ›´é•¿ï¼Œæˆªæ–­åˆ°æœŸæœ›é•¿åº¦
                if window_len > expected_window_len:
                    window_emb = window_emb[:expected_window_len]
                    window_len = expected_window_len
            
            actual_end = min(start_idx + window_len, full_length)
            actual_len = actual_end - start_idx
            
            # ç¡®ä¿ä¸è¶…è¿‡ merged embeddings tensor
            if actual_end > full_length:
                actual_end = full_length
                actual_len = full_length - start_idx
            
            # ç´¯åŠ  embeddings å’Œè®¡æ•°
            merged_embeddings[start_idx:actual_end] += window_emb[:actual_len]
            count_tensor[start_idx:actual_end] += 1
        
        # è®¡ç®—å¹³å‡å€¼ï¼šæ¯ä¸ªä½ç½®çš„å€¼ = æ‰€æœ‰è¦†ç›–è¯¥ä½ç½®çš„ windows çš„å¹³å‡å€¼
        count_tensor = torch.clamp(count_tensor, min=1.0)
        merged_embeddings = merged_embeddings / count_tensor.unsqueeze(-1)
        
        # éªŒè¯æœ€ç»ˆé•¿åº¦åŒ¹é…æœŸæœ›
        final_len = merged_embeddings.shape[0]
        if final_len != full_length:
            raise RuntimeError(
                f"âŒ Merged embedding length ({final_len}) != expected length ({full_length})"
            )
        
        return merged_embeddings

    def prepare_data_for_training(self, model_name: str, layer_index: int, batch_size: int,
                                task_name: str, adaptor_checkpoint: Optional[str] = None,
                                device: str = "cuda") -> Dict[str, Any]:
        """
        å‡†å¤‡ç”¨äºè®­ç»ƒçš„æ•°æ®ï¼ŒåŒ…æ‹¬åŠ è½½embeddingså¹¶é€šè¿‡adaptorå¤„ç†ã€‚
        æ ¹æ®ä»»åŠ¡åç§°è°ƒç”¨å¯¹åº”çš„ä»»åŠ¡ç‰¹å®šå¤„ç†æ–¹æ³•ã€‚

        @param model_name: æ¨¡å‹åç§°
        @param layer_index: å±‚ç´¢å¼•
        @param batch_size: Batch size for adaptor processing
        @param task_name: ä»»åŠ¡åç§° ('ppi', 'nhas', 'p_site' ç­‰)
        @param adaptor_checkpoint: adaptor checkpointåç§°ã€‚å¦‚æœä¸º Noneï¼Œç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ embeddings
        @param device: è®¾å¤‡
        @return: å¤„ç†åçš„æ•°æ®å­—å…¸
        """
        # åŠ è½½åŸå§‹embeddingså’Œmetadata
        raw_embeddings, metadata_dict = self.load_embeddings_for_task(model_name, layer_index, task_name)

        # æ ¹æ®ä»»åŠ¡åç§°è°ƒç”¨å¯¹åº”çš„å¤„ç†æ–¹æ³•
        if task_name == 'ppi':
            processed_data = self._prepare_ppi_data(
                raw_embeddings, metadata_dict, batch_size, adaptor_checkpoint, device
            )
        elif task_name in ['nhas', 'p_site']:
            # NHAå’ŒP-siteä»»åŠ¡ä½¿ç”¨ç›¸åŒçš„å¤„ç†é€»è¾‘ï¼ˆåºåˆ—çº§åˆ«çš„embeddingsï¼‰
            processed_data = self._prepare_sequence_level_data(
                raw_embeddings, metadata_dict, batch_size, adaptor_checkpoint, device
            )
        else:
            raise ValueError(f"Unknown task_name: {task_name}. Supported tasks: 'ppi', 'nhas', 'p_site'")

        print("âœ… Data preparation completed!")
        return processed_data
    
    def _process_single_embedding_type(self, raw_embeddings: Dict[str, Any], metadata_dict: Dict[str, Any],
                                     embedding_key: str, batch_size: int, adaptor_checkpoint: Optional[str],
                                     device: str, need_pooling: bool = False, return_sequence_ids: bool = False) -> Optional[List[torch.Tensor]]:
        """
        å¤„ç†å•ä¸ªembeddingç±»å‹ï¼šé€šè¿‡adaptorå¤„ç†ï¼Œmerge windowsï¼Œç§»é™¤ç‰¹æ®Štokenã€‚
        è¿™æ˜¯æ‰€æœ‰ä»»åŠ¡å…±ç”¨çš„æ ¸å¿ƒå¤„ç†é€»è¾‘ã€‚

        @param raw_embeddings: åŸå§‹embeddingså­—å…¸
        @param metadata_dict: Metadataå­—å…¸
        @param embedding_key: Embeddingçš„keyï¼ˆå¦‚ 'train_binder_embeddings'ï¼‰
        @param batch_size: Batch size for adaptor processing
        @param adaptor_checkpoint: Adaptor checkpointåç§°
        @param device: è®¾å¤‡
        @param need_pooling: æ˜¯å¦éœ€è¦æ± åŒ–ï¼ˆPPIä»»åŠ¡éœ€è¦ï¼ŒNHAä»»åŠ¡ä¸éœ€è¦ï¼‰
        @return: å¤„ç†åçš„embeddingsåˆ—è¡¨ï¼Œå¦‚æœkeyä¸å­˜åœ¨åˆ™è¿”å›None
        """
        if embedding_key not in raw_embeddings:
            return None
        
        metadata = metadata_dict.get(embedding_key)
        if metadata is None:
            raise ValueError(f"Metadata not found for {embedding_key}")
        
        # ä½¿ç”¨å¯¹åº”çš„metadataè¿›è¡Œmergeï¼Œè¿”å› List[torch.Tensor]
        # æ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º [seq_len, hidden_size]ï¼ŒæŒ‰sequence_idé¡ºåºæ’åˆ—
        sequence_embeddings = self.process_embeddings(
            raw_embeddings[embedding_key],
            metadata_list=metadata,
            batch_size=batch_size,
            adaptor_checkpoint=adaptor_checkpoint,
            device=device
        )
        
        # å¦‚æœéœ€è¦æ± åŒ–ï¼Œè½¬æ¢ä¸ºå›ºå®šå¤§å°çš„å‘é‡ [hidden_size]
        if need_pooling:
            sequence_embeddings = self._pool_sequence_embeddings(
                sequence_embeddings,
                pool_method='mean'
            )
        
        return sequence_embeddings
    
    def _prepare_ppi_data(self, raw_embeddings: Dict[str, Any], metadata_dict: Dict[str, Any],
                          batch_size: int, adaptor_checkpoint: Optional[str], device: str) -> Dict[str, Any]:
        """
        å‡†å¤‡PPIä»»åŠ¡çš„æ•°æ®ã€‚
        PPIä»»åŠ¡ç‰¹æ®Šå¤„ç†ï¼š
        - binderå’Œwtï¼šç›´æ¥ä½¿ç”¨loadè¿›æ¥çš„åŸå§‹embeddings
          * éœ€è¦merge windowsï¼ˆå¦‚æœæœ‰å¤šä¸ªwindowsï¼‰
          * éœ€è¦ç§»é™¤ç‰¹æ®Štoken
          * ä¸ç»è¿‡adaptor blockå¤„ç†
        - ptmï¼šä½¿ç”¨loadè¿›æ¥çš„PTM embeddings
          * éœ€è¦merge windowsï¼ˆå¦‚æœæœ‰å¤šä¸ªwindowsï¼‰
          * éœ€è¦ç§»é™¤ç‰¹æ®Štoken
          * éœ€è¦ç»è¿‡adaptor blockå¤„ç†
        
        æ‰€æœ‰embeddingséƒ½éœ€è¦æ± åŒ–ä¸ºå›ºå®šå¤§å°çš„å‘é‡ã€‚

        @param raw_embeddings: åŸå§‹embeddingså­—å…¸ï¼ˆåŒ…å«binderã€wtå’Œptm embeddingsï¼‰
        @param metadata_dict: Metadataå­—å…¸
        @param batch_size: Batch size for adaptor processing
        @param adaptor_checkpoint: Adaptor checkpointåç§°ï¼ˆä»…ç”¨äºPTMï¼‰
        @param device: è®¾å¤‡
        @return: å¤„ç†åçš„æ•°æ®å­—å…¸
        """
        processed_data = {}
        
        # ğŸ”§ PPIä»»åŠ¡ï¼šå¤„ç†åŸå§‹åºåˆ—ï¼ˆbinderå’Œwtï¼‰- ç›´æ¥ä½¿ç”¨loadè¿›æ¥çš„embeddingsï¼Œä¸ç»è¿‡adaptor block
        original_embedding_types = [
            'train_binder_embeddings', 'train_wt_embeddings',
            'valid_binder_embeddings', 'valid_wt_embeddings',
            'test_binder_embeddings', 'test_wt_embeddings'
        ]
        
        # ğŸ”§ PPIä»»åŠ¡ï¼šå¤„ç†PTM embeddings - ä½¿ç”¨loadè¿›æ¥çš„embeddingsï¼Œç„¶åé€šè¿‡adaptor blockå¤„ç†
        ptm_embedding_types = [
            'train_ptm_embeddings',
            'valid_ptm_embeddings',
            'test_ptm_embeddings'
        ]
        
        # è®¡ç®—æ€»æ­¥éª¤æ•°ï¼ˆåŸå§‹åºåˆ— + PTM embeddingsï¼‰
        total_steps = sum(1 for key in original_embedding_types if key in raw_embeddings)
        total_steps += sum(1 for key in ptm_embedding_types if key in raw_embeddings)
        
        with tqdm(total=total_steps, desc="Processing PPI embeddings", unit="split") as pbar:
            # ğŸ”§ å¤„ç†åŸå§‹åºåˆ—çš„embeddingsï¼ˆbinderå’Œwtï¼‰
            # - éœ€è¦merge windowsï¼ˆå¦‚æœæœ‰å¤šä¸ªwindowsï¼‰
            # - éœ€è¦ç§»é™¤ç‰¹æ®Štoken
            # - ä¸ç»è¿‡adaptor blockå¤„ç†
            for embedding_key in original_embedding_types:
                if embedding_key not in raw_embeddings:
                    continue
                
                metadata = metadata_dict.get(embedding_key)
                if metadata is None:
                    raise ValueError(f"Metadata not found for {embedding_key}")
                
                # å¤„ç†embeddingsï¼šmerge windows + ç§»é™¤ç‰¹æ®Štokenï¼Œä½†ä¸ç»è¿‡adaptor block
                sequence_embeddings = self.process_embeddings(
                    raw_embeddings[embedding_key],
                    metadata_list=metadata,
                    batch_size=batch_size,
                    adaptor_checkpoint=None,  # ä¸ç»è¿‡adaptor block
                    device=device
                )
                
                # ğŸ”§ Mean pooling: å°†åºåˆ—çº§embeddingsæ± åŒ–ä¸ºå›ºå®šå¤§å°çš„å‘é‡
                mean_embeddings = self._pool_sequence_embeddings(
                    sequence_embeddings,
                    pool_method='mean'
                )
                
                processed_data[embedding_key] = mean_embeddings
                pbar.update(1)
            
            # ğŸ”§ å¤„ç†PTM embeddings - ä½¿ç”¨loadè¿›æ¥çš„embeddingsï¼Œç„¶åé€šè¿‡adaptor blockå¤„ç†
            for embedding_key in ptm_embedding_types:
                if embedding_key not in raw_embeddings:
                    continue
                
                metadata = metadata_dict.get(embedding_key)
                if metadata is None:
                    raise ValueError(f"Metadata not found for {embedding_key}")
                
                # å¤„ç†PTM embeddingsï¼ˆé€šè¿‡adaptor blockå¤„ç†ï¼‰
                sequence_embeddings = self.process_embeddings(
                    raw_embeddings[embedding_key],
                    metadata_list=metadata,
                    batch_size=batch_size,
                    adaptor_checkpoint=adaptor_checkpoint,  # ç»è¿‡adaptor block
                    device=device
                )
                
                # ğŸ”§ Mean pooling: å°†åºåˆ—çº§embeddingsæ± åŒ–ä¸ºå›ºå®šå¤§å°çš„å‘é‡
                mean_embeddings = self._pool_sequence_embeddings(
                    sequence_embeddings,
                    pool_method='mean'
                )
                
                processed_data[embedding_key] = mean_embeddings
                pbar.update(1)
            
            # å¤åˆ¶labelsï¼ˆé¡ºåºä¸embeddingså¯¹åº”ï¼‰
            for split in ['train', 'valid', 'test']:
                labels_key = f'{split}_labels'
                if labels_key in raw_embeddings:
                    processed_data[labels_key] = raw_embeddings[labels_key]
         
        return processed_data
    
    def _prepare_sequence_level_data(self, raw_embeddings: Dict[str, Any], metadata_dict: Dict[str, Any],
                                    batch_size: int, adaptor_checkpoint: Optional[str], device: str) -> Dict[str, Any]:
        """
        å‡†å¤‡åºåˆ—çº§åˆ«ä»»åŠ¡çš„æ•°æ®ï¼ˆå¦‚NHAã€P-siteï¼‰ã€‚
        è¿™äº›ä»»åŠ¡éœ€è¦ä¿æŒåºåˆ—çº§åˆ«çš„embeddingsï¼ˆä¸æ± åŒ–ï¼‰ï¼Œç”¨äºä½ç½®çº§åˆ«çš„é¢„æµ‹ã€‚

        @param raw_embeddings: åŸå§‹embeddingså­—å…¸
        @param metadata_dict: Metadataå­—å…¸
        @param batch_size: Batch size for adaptor processing
        @param adaptor_checkpoint: Adaptor checkpointåç§°
        @param device: è®¾å¤‡
        @return: å¤„ç†åçš„æ•°æ®å­—å…¸
        """
        processed_data = {}
        
        # åºåˆ—çº§åˆ«ä»»åŠ¡çš„embeddingç±»å‹åˆ—è¡¨
        embedding_types = ['train_embeddings', 'valid_embeddings', 'test_embeddings']
        
        # è®¡ç®—æ€»æ­¥éª¤æ•°
        total_steps = sum(1 for key in embedding_types if key in raw_embeddings)
        
        with tqdm(total=total_steps, desc="Processing sequence-level embeddings", unit="split") as pbar:
            # å¤„ç†æ¯ç§ç±»å‹çš„embeddings
            for embedding_key in embedding_types:
                processed_embeddings = self._process_single_embedding_type(
                    raw_embeddings, metadata_dict, embedding_key,
                    batch_size, adaptor_checkpoint, device,
                    need_pooling=False
                )
                if processed_embeddings is not None:
                    processed_data[embedding_key] = processed_embeddings
                    # å¤åˆ¶å¯¹åº”çš„labels
                    labels_key = embedding_key.replace('_embeddings', '_labels')
                    if labels_key in raw_embeddings:
                        processed_data[labels_key] = raw_embeddings[labels_key]
                    pbar.update(1)
        
        return processed_data
    
    def _pool_sequence_embeddings(self, embeddings_list: List[torch.Tensor], pool_method: str = 'mean') -> List[torch.Tensor]:
        """
        å¯¹åºåˆ—çº§åˆ«çš„embeddingsè¿›è¡Œæ± åŒ–ï¼Œå°† [seq_len, hidden_size] è½¬æ¢ä¸º [hidden_size]ã€‚
        
        @param embeddings_list: List of embeddingsï¼Œæ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º [seq_len, hidden_size]ï¼ŒæŒ‰sequenceé¡ºåºæ’åˆ—
        @param pool_method: æ± åŒ–æ–¹æ³•ï¼Œ'mean' æˆ– 'max'
        @return: List of pooled embeddingsï¼Œæ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º [hidden_size]ï¼Œé¡ºåºä¿æŒä¸å˜
        """
        pooled_embeddings = []
        for emb in embeddings_list:
            if emb.dim() != 2:
                raise ValueError(f"Expected 2D tensor [seq_len, hidden_size], got {emb.dim()}D tensor with shape {emb.shape}")
            
            if pool_method == 'mean':
                # Mean pooling: å¯¹åºåˆ—ç»´åº¦æ±‚å¹³å‡
                pooled = emb.mean(dim=0)  # [hidden_size]
            elif pool_method == 'max':
                # Max pooling: å¯¹åºåˆ—ç»´åº¦æ±‚æœ€å¤§å€¼
                pooled = emb.max(dim=0)[0]  # [hidden_size]
            else:
                raise ValueError(f"Unknown pool_method: {pool_method}. Use 'mean' or 'max'.")
            
            pooled_embeddings.append(pooled)
        
        return pooled_embeddings
    
    def _remove_special_tokens(self, embeddings: torch.Tensor) -> torch.Tensor:
        """
        ç§»é™¤ç‰¹æ®Š tokenï¼ˆESM2: <cls> å’Œ <eos>ï¼ŒESM-C: BOS å’Œ EOSï¼‰ã€‚
        åœ¨ adaptor block å¤„ç†å®Œæˆåï¼Œç§»é™¤ç‰¹æ®Š token ä½ç½®ã€‚
        
        @param embeddings: Embedding tensor with shape (seq_len + 2, hidden_size) æˆ– (seq_len + 1, hidden_size)
        @returns: Embedding tensor with shape (seq_len, hidden_size)ï¼Œå·²ç§»é™¤ç‰¹æ®Š token
        """
        if embeddings.shape[0] > 2:
            # ç§»é™¤ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ª tokenï¼ˆ<cls>/BOS å’Œ <eos>/EOSï¼‰
            return embeddings[1:-1]
        elif embeddings.shape[0] == 2:
            # åªæœ‰ä¸¤ä¸ª tokenï¼Œä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆé€šå¸¸æ˜¯ <cls>/BOSï¼‰
            return embeddings[0:1]
        else:
            # åªæœ‰ä¸€ä¸ª tokenï¼Œç›´æ¥è¿”å›
            return embeddings