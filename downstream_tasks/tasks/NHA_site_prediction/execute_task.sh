#!/bin/bash
# NHA Site Prediction Pipeline
# NHAä½ç‚¹é¢„æµ‹å®Œæ•´æµç¨‹è„šæœ¬

# æ£€æŸ¥å‚æ•°
if [ $# -ne 4 ]; then
    echo "Usage: $0 <adaptor_checkpoint> <model_name> <layer> <gpu_device>"
    echo "Example: $0 LoRA_combine EvolutionaryScale_esmc-600m-2024-12 30 1"
    echo "Example (no checkpoint): $0 None EvolutionaryScale_esmc-600m-2024-12 30 1"
    exit 1
fi

ADAPTOR_CHECKPOINT="$1"
# å¤„ç†ç©ºå­—ç¬¦ä¸²æˆ– "None" å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºçœŸæ­£çš„ None
if [ -z "$ADAPTOR_CHECKPOINT" ] || [ "$ADAPTOR_CHECKPOINT" = "None" ] || [ "$ADAPTOR_CHECKPOINT" = "none" ]; then
    ADAPTOR_CHECKPOINT="None"
fi
MODEL_NAME="$2"
LAYER_INDEX="$3"
GPU_DEVICE="$4"

# æ¿€æ´»ç¯å¢ƒ
source /home/zz/miniconda3/etc/profile.d/conda.sh
conda activate ptm

set -e  # Exit on error
export CUDA_VISIBLE_DEVICES="${GPU_DEVICE}"

# ============================================
# é…ç½®å‚æ•° (Configuration)
# ============================================
WORK_DIR="/home/zz/zheng/ptm-mlm/downstream_tasks/tasks/NHA_site_prediction"
BASE_OUTPUT_DIR="/home/zz/zheng/ptm-mlm/downstream_tasks/outputs"

# åˆ›å»ºå¸¦æ—¥æœŸçš„è¾“å‡ºç›®å½•
DATE_STR=$(date +"%Y-%m-%d")
OUTPUT_DIR="${BASE_OUTPUT_DIR}/NHA_site_prediction_${ADAPTOR_CHECKPOINT}_${DATE_STR}"

# è®­ç»ƒå‚æ•°
BATCH_SIZE=512
NUM_EPOCHS=10
LEARNING_RATE=1e-4
DEVICE="cuda"
LAMBDA_WEIGHT=0.1
TRAIN_BY_LENGTH_GROUPS=true

echo "ğŸ”„ Starting NHA site prediction pipeline..."
if [ -z "$ADAPTOR_CHECKPOINT" ] || [ "$ADAPTOR_CHECKPOINT" = "None" ] || [ "$ADAPTOR_CHECKPOINT" = "none" ]; then
    echo "   Adaptor checkpoint: None (using raw embeddings)"
else
    echo "   Adaptor checkpoint: ${ADAPTOR_CHECKPOINT}"
fi
echo "   Model name: ${MODEL_NAME}"
echo "   Layer index: ${LAYER_INDEX}"
echo "   GPU device: ${GPU_DEVICE}"
echo "   Output dir: ${OUTPUT_DIR}"

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p "${OUTPUT_DIR}"

cd "${WORK_DIR}"

# ğŸ”§ æ„å»º Python å‘½ä»¤å‚æ•°
PYTHON_ARGS=(
    --output_dir "${OUTPUT_DIR}"
    --num_epochs ${NUM_EPOCHS}
    --batch_size ${BATCH_SIZE}
    --learning_rate ${LEARNING_RATE}
    --device "${DEVICE}"
    --lambda_weight ${LAMBDA_WEIGHT}
)

# ğŸ”§ model_name å¿…é¡»ä¼ é€’ï¼ˆä¸èƒ½ä¸ºç©ºï¼‰
if [ -z "$MODEL_NAME" ]; then
    echo "âŒ é”™è¯¯: model_name ä¸èƒ½ä¸ºç©º!"
    exit 1
fi
PYTHON_ARGS+=(--model_name "${MODEL_NAME}")

# ğŸ”§ åªæœ‰å½“ layer_index ä¸ä¸ºç©ºä¸”ä¸æ˜¯ "None" æ—¶æ‰æ·»åŠ ï¼ˆå› ä¸º Python æœŸæœ› int ç±»å‹ï¼‰
if [ -n "$LAYER_INDEX" ] && [ "$LAYER_INDEX" != "None" ] && [ "$LAYER_INDEX" != "none" ]; then
    PYTHON_ARGS+=(--layer_index ${LAYER_INDEX})
fi

# ğŸ”§ åªæœ‰å½“ adaptor_checkpoint ä¸ä¸ºç©ºä¸”ä¸æ˜¯ "None" æ—¶æ‰æ·»åŠ 
if [ -n "$ADAPTOR_CHECKPOINT" ] && [ "$ADAPTOR_CHECKPOINT" != "None" ] && [ "$ADAPTOR_CHECKPOINT" != "none" ]; then
    PYTHON_ARGS+=(--adaptor_checkpoint "${ADAPTOR_CHECKPOINT}")
fi

# ğŸ”§ æ·»åŠ  train_by_length_groups æ ‡å¿—ï¼ˆå¦‚æœéœ€è¦ï¼‰
if [ "${TRAIN_BY_LENGTH_GROUPS}" = "true" ]; then
    PYTHON_ARGS+=(--train_by_length_groups)
fi

python3 train_and_evaluation.py "${PYTHON_ARGS[@]}"

echo "âœ… Step 2 å®Œæˆ: åˆ†ç±»å¤´è®­ç»ƒå®Œæˆï¼Œæµ‹è¯•é›†è¯„ä¼°å®Œæˆ"
echo ""

# ============================================
# ğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæ¯•!
# ============================================
echo "============================================"
echo "ğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæ¯•!"
echo "============================================"
echo "è¾“å‡ºæ–‡ä»¶:"
if [ "${TRAIN_BY_LENGTH_GROUPS}" = "true" ]; then
    echo "  - æŒ‰é•¿åº¦åˆ†ç»„çš„æ¨¡å‹: ${OUTPUT_DIR}/length_*/ (æ¯ä¸ªé•¿åº¦ä¸€ä¸ªç‹¬ç«‹æ¨¡å‹)"
    echo "  - é•¿åº¦åˆ†ç»„æ‘˜è¦: ${OUTPUT_DIR}/length_groups_summary.json"
else
    echo "  - è®­ç»ƒå¥½çš„æ¨¡å‹: ${OUTPUT_DIR}/trained_head.pt"
fi
echo "  - è¾“å‡ºç›®å½•: ${OUTPUT_DIR}/"
echo "============================================"

