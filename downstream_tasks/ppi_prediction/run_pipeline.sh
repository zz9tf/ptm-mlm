#!/bin/bash
# PPI Prediction Pipeline
# è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨é¢„æµ‹å®Œæ•´æµç¨‹è„šæœ¬

source /home/zz/miniconda3/etc/profile.d/conda.sh
conda activate ptm
cd /home/zz/zheng/ptm-mlm/main_pipeline

set -e  # Exit on error
export CUDA_VISIBLE_DEVICES="4"

# ============================================
# é…ç½®å‚æ•° (Configuration)
# ============================================
WORK_DIR="/home/zz/zheng/ptm-mlm/downstream_tasks/ppi_prediction"
CHECKPOINT="${WORK_DIR}/../checkpoints/LoRA_ptm.ckpt"  # LoRAæ¨¡å‹checkpointè·¯å¾„
DATA="${WORK_DIR}/PTM experimental evidence.csv"  # PPIæ•°æ®æ–‡ä»¶
BASE_OUTPUT_DIR="/home/zz/zheng/ptm-mlm/downstream_tasks/outputs"  # åŸºç¡€è¾“å‡ºç›®å½•
# åˆ›å»ºå¸¦æ—¥æœŸçš„è¾“å‡ºç›®å½•
DATE_STR=$(date +"%Y-%m-%d")
OUTPUT_DIR="${BASE_OUTPUT_DIR}/ppi_prediction_lora_ptm_${DATE_STR}"
BATCH_SIZE=512
NUM_EPOCHS=50
LEARNING_RATE=1e-4
DEVICE="cuda"  # æˆ– "cpu"
MAX_SEQUENCE_LENGTH=512  # æœ€å¤§åºåˆ—é•¿åº¦
MAX_LENGTH=2000  # TransformerClassifierçš„æœ€å¤§é•¿åº¦å‚æ•°
DROPOUT=0.3  # Dropout rate
MODEL_TYPE="esmc"  # æ¨¡å‹ç±»å‹: "mamba", "esm2", "lora", æˆ– "esmc"
USE_ESM=true  # æ˜¯å¦åŠ è½½ESM2-15Bæ¨¡å‹ï¼ˆä»…åœ¨MODEL_TYPE="mamba"æ—¶æœ‰æ•ˆï¼‰
                # true: ä½¿ç”¨Mamba+ESM2-15Bç»„åˆï¼ˆè‡ªåŠ¨åŠ è½½esm2_t48_15B_UR50Dï¼ŒåŒ¹é…è®­ç»ƒæ—¶çš„é…ç½®ï¼‰
                # false: åªä½¿ç”¨Mambaæ¨¡å‹ï¼ˆä¸åŠ è½½ESMï¼ŒèŠ‚çœå†…å­˜ï¼‰
TRAIN_RATIO=0.7  # è®­ç»ƒé›†æ¯”ä¾‹
VALID_RATIO=0.15  # éªŒè¯é›†æ¯”ä¾‹
TEST_RATIO=0.15  # æµ‹è¯•é›†æ¯”ä¾‹
RANDOM_SEED=42  # éšæœºç§å­

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p "${OUTPUT_DIR}"

cd "${WORK_DIR}"

# # ============================================
# # Step 1: ç”ŸæˆEmbeddings (Generate Embeddings)
# # ============================================
# echo "============================================"
# echo "Step 1: ç”ŸæˆEmbeddings (Binder, WT, PTM)"
# echo "============================================"

# python3 generate_embeddings.py \
#     --model_type "${MODEL_TYPE}" \
#     --data "${DATA}" \
#     --output_dir "${OUTPUT_DIR}" \
#     --batch_size ${BATCH_SIZE} \
#     --max_sequence_length ${MAX_SEQUENCE_LENGTH} \
#     --train_ratio ${TRAIN_RATIO} \
#     --valid_ratio ${VALID_RATIO} \
#     --test_ratio ${TEST_RATIO} \
#     --random_seed ${RANDOM_SEED}

# echo "âœ… Step 1 å®Œæˆ: Embeddingså·²ç”Ÿæˆ"
# echo ""

# ============================================
# Step 2: è®­ç»ƒåˆ†ç±»å¤´å¹¶è¯„ä¼° (Train Classification Head and Evaluate)
# ============================================
echo "============================================"
echo "Step 2: è®­ç»ƒTransformerClassifierå¹¶è¯„ä¼°"
echo "============================================"

python3 train_and_evaluation.py \
    --output_dir "${OUTPUT_DIR}" \
    --num_epochs ${NUM_EPOCHS} \
    --batch_size ${BATCH_SIZE} \
    --learning_rate ${LEARNING_RATE} \
    --dropout ${DROPOUT} \
    --max_length ${MAX_LENGTH} \
    --device "${DEVICE}"

echo "âœ… Step 2 å®Œæˆ: åˆ†ç±»å¤´è®­ç»ƒå®Œæˆï¼Œæµ‹è¯•é›†è¯„ä¼°å®Œæˆ"
echo ""

# ============================================
# ğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæ¯•!
# ============================================
echo "============================================"
echo "ğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæ¯•!"
echo "============================================"
echo "è¾“å‡ºæ–‡ä»¶:"
echo "  - Embeddings: ${OUTPUT_DIR}/embeddings/"
echo "    * train_binder_embeddings.pt, train_wt_embeddings.pt, train_ptm_embeddings.pt"
echo "    * valid_binder_embeddings.pt, valid_wt_embeddings.pt, valid_ptm_embeddings.pt"
echo "    * test_binder_embeddings.pt, test_wt_embeddings.pt, test_ptm_embeddings.pt"
echo "    * train_labels.pt, valid_labels.pt, test_labels.pt"
echo "  - è®­ç»ƒå¥½çš„æ¨¡å‹: ${OUTPUT_DIR}/best_model.pt"
echo "  - è®­ç»ƒå†å²: ${OUTPUT_DIR}/training_history.json"
echo "  - è®­ç»ƒæ›²çº¿: ${OUTPUT_DIR}/training_curves.png"
echo "  - æµ‹è¯•æŒ‡æ ‡: ${OUTPUT_DIR}/test_metrics.json"
echo "  - è¾“å‡ºç›®å½•: ${OUTPUT_DIR}/"
echo "============================================"

