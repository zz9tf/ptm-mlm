"""
LoRA æ¨¡å‹æ¨ç†è„šæœ¬ï¼Œç”¨äºä»é¢„è®­ç»ƒçš„ LoRA checkpoint ç”Ÿæˆ block è¾“å‡ºã€‚
æ­¤è„šæœ¬åŠ è½½ LoRA checkpoint å¹¶ä»…è¿”å› block çš„è¾“å‡ºç»“æœï¼ˆä¸ç»è¿‡ headsï¼‰ã€‚

å¿…é¡»ä½¿ç”¨ ESM C 600M æ¨¡å‹ç”Ÿæˆ embeddingsã€‚

è¿™æ˜¯ä¸€ä¸ªå…±äº«æ¨¡å—ï¼Œç”¨äºæ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡ã€‚
"""
import torch
from tqdm import tqdm
import sys
from pathlib import Path

# æ·»åŠ ä¸»é¡¹ç›®è·¯å¾„ä»¥å¯¼å…¥æ¨¡å‹
_main_pipeline_path = Path(__file__).parent.parent.parent / "main_pipeline"
if str(_main_pipeline_path) not in sys.path:
    sys.path.insert(0, str(_main_pipeline_path))

from models.model import PTMModel
from getters.tokenizer import PTMTokenizer


class LoRAInference:
    """
    LoRA æ¨¡å‹æ¨ç†ç±»ï¼Œç”¨äºä»é¢„è®­ç»ƒçš„ LoRA checkpoint ç”Ÿæˆ block è¾“å‡ºã€‚
    ä»…è¿”å› block çš„è¾“å‡ºç»“æœï¼Œä¸ç»è¿‡ headsã€‚
    å¿…é¡»ä½¿ç”¨ ESM C 600M æ¨¡å‹ã€‚
    """
    
    def __init__(self, checkpoint_path: str, device: str = None, max_sequence_length: int = None):
        """
        åˆå§‹åŒ– LoRA æ¨ç†æ¨¡å‹ã€‚
        
        @param checkpoint_path: è®­ç»ƒå¥½çš„æ¨¡å‹ checkpoint è·¯å¾„ï¼ˆ.ckpt æ–‡ä»¶ï¼‰
        @param device: è¿è¡Œæ¨ç†çš„è®¾å¤‡ï¼ˆNone è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼‰
        @param max_sequence_length: tokenization çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚
                                   å¦‚æœä¸º Noneï¼Œåºåˆ—ä¸ä¼šè¢«æˆªæ–­ï¼ˆå¯èƒ½å¯¼è‡´å†…å­˜é—®é¢˜ï¼‰ã€‚
                                   é»˜è®¤: 512ï¼ˆåŒ¹é…è®­ç»ƒé…ç½®ï¼‰
        """
        # ç¡®å®šè®¾å¤‡
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # åŠ è½½ tokenizer
        self.tokenizer = PTMTokenizer()
        
        # åŠ è½½ LoRA checkpoint
        print(f"ğŸ“¦ æ­£åœ¨ä» {checkpoint_path} åŠ è½½ LoRA æ¨¡å‹...")
        ckpt = torch.load(checkpoint_path, map_location='cpu')
        model_state_dict = ckpt["model"]
        model_config_dict = ckpt["config"]
        
        # ä»é…ç½®ä¸­è·å–æ¨¡å‹å‚æ•°
        # ESM C 600M çš„é»˜è®¤ç»´åº¦æ˜¯ 1152
        embed_dim = model_config_dict.get("embed_dim", 1152)
        vocab_size = model_config_dict.get("vocab_size", self.tokenizer.get_vocab_size())
        d_model = model_config_dict.get("d_model", 512)
        block_config = model_config_dict.get("block_config", {"type": "lora"})
        
        # åˆå§‹åŒ– PTMModelï¼ˆåªä½¿ç”¨ blockï¼Œheads ä¸ä¼šè¢«ä½¿ç”¨ï¼‰
        self.model = PTMModel(
            embed_dim=embed_dim,
            vocab_size=vocab_size,
            d_model=d_model,
            block_config=block_config,
            heads_config=[],  # ä¸åˆ›å»º headsï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€è¦ block è¾“å‡º
            device=self.device,
        )
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€
        msg = self.model.load_state_dict(model_state_dict, strict=False)
        print(f"ğŸ“ æ¨¡å‹åŠ è½½ä¿¡æ¯: {msg}")
        
        # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        self.model = self.model.to(self.device)
        self.model.eval()
        
        # åŠ è½½ ESM C 600M æ¨¡å‹ï¼ˆå¿…é¡»ä½¿ç”¨ï¼‰
        print(f"ğŸ“¦ æ­£åœ¨åŠ è½½ ESM C 600M æ¨¡å‹...")
        try:
            # é¦–å…ˆæ£€æŸ¥ esm æ¨¡å—æ˜¯å¦å¯ç”¨
            try:
                import esm
            except ImportError:
                raise ImportError(
                    "âŒ esm æ¨¡å—æœªå®‰è£…ã€‚è¯·å®‰è£… ESM åº“ï¼š\n"
                    "   pip install fair-esm\n"
                    "   æˆ–è€…ä»æºç å®‰è£…ï¼š\n"
                    "   git clone https://github.com/facebookresearch/esm.git\n"
                    "   cd esm && pip install -e ."
                )
            
            # æ£€æŸ¥ esm.models æ¨¡å—æ˜¯å¦å­˜åœ¨
            try:
                from esm.models.esmc import ESMC
            except (ImportError, AttributeError) as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å—ç»“æ„é—®é¢˜
                import importlib
                esm_module = importlib.import_module('esm')
                esm_path = getattr(esm_module, '__path__', [None])[0]
                raise ImportError(
                    f"âŒ æ— æ³•å¯¼å…¥ esm.models.esmc æ¨¡å—: {e}\n"
                    f"   esm æ¨¡å—è·¯å¾„: {esm_path}\n"
                    f"   è¯·ç¡®ä¿å®‰è£…çš„ esm åº“ç‰ˆæœ¬æ”¯æŒ ESM C æ¨¡å‹ã€‚\n"
                    f"   å¯èƒ½éœ€è¦å®‰è£…ç‰¹å®šç‰ˆæœ¬ï¼š\n"
                    f"   pip install 'fair-esm>=2.0.0' æˆ–ä»æºç å®‰è£…æœ€æ–°ç‰ˆæœ¬"
                )
            
            # åŠ è½½ ESM C 600M æ¨¡å‹
            self.esm_model = ESMC.from_pretrained("esmc_600m")
            self.esm_model = self.esm_model.to(self.device)
            self.esm_model.eval()
            for param in self.esm_model.parameters():
                param.requires_grad = False
            
            # ESM C ä½¿ç”¨ä¸åŒçš„ API
            try:
                from esm.sdk.api import ESMProtein, LogitsConfig
            except (ImportError, AttributeError) as e:
                raise ImportError(
                    f"âŒ æ— æ³•å¯¼å…¥ esm.sdk.api æ¨¡å—: {e}\n"
                    f"   è¯·ç¡®ä¿å®‰è£…çš„ esm åº“ç‰ˆæœ¬æ”¯æŒ ESM C SDK APIã€‚"
                )
            
            self.ESMProtein = ESMProtein
            self.LogitsConfig = LogitsConfig
            self.esm_layer = 30  # ä½¿ç”¨ç¬¬30å±‚ï¼ˆå¿…é¡»ä½¿ç”¨ï¼‰
            
            print(f"âœ… ESM C 600M æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            print(f"ğŸ“Œ ä½¿ç”¨ç¬¬ {self.esm_layer} å±‚çš„è¾“å‡º")
        except ImportError as e:
            # ImportError ç›´æ¥æŠ›å‡ºï¼Œå› ä¸ºå·²ç»åŒ…å«äº†è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            raise
        except Exception as e:
            raise RuntimeError(
                f"âŒ æ— æ³•åŠ è½½ ESM C 600M æ¨¡å‹: {e}\n"
                f"   é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"   è¯·æ£€æŸ¥ï¼š\n"
                f"   1. esm åº“æ˜¯å¦æ­£ç¡®å®‰è£…\n"
                f"   2. esm åº“ç‰ˆæœ¬æ˜¯å¦æ”¯æŒ ESM C 600M\n"
                f"   3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½æ¨¡å‹ï¼‰\n"
                f"   å®‰è£…å‘½ä»¤ï¼špip install fair-esm"
            )
        
        # ä»æ¨¡å‹é…ç½®è·å–éšè—å±‚å¤§å°
        self.hidden_size = d_model
        
        # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ 512ï¼ŒåŒ¹é…è®­ç»ƒé…ç½®ï¼‰
        if max_sequence_length is None:
            self.max_sequence_length = getattr(self.model, 'max_sequence_length', 512)
        else:
            self.max_sequence_length = max_sequence_length
        
        print(f"âœ… LoRA æ¨¡å‹åŠ è½½æˆåŠŸï¼éšè—å±‚å¤§å°: {self.hidden_size}")
        print(f"ğŸ“ æœ€å¤§åºåˆ—é•¿åº¦: {self.max_sequence_length}")
        print(f"ğŸ”§ æ¨¡å¼: LoRA Block (ä»… block è¾“å‡ºï¼Œä¸ç»è¿‡ heads)")
    
    @torch.no_grad()
    def _compute_esmc_embedding(self, sequences: list):
        """
        ä½¿ç”¨ ESM C 600M è®¡ç®— embeddingsã€‚
        
        @param sequences: è›‹ç™½è´¨åºåˆ—åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²ï¼‰
        @returns: ESM C embeddings å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, embed_dim)
        """
        batch_embeddings = []
        
        for seq in sequences:
            try:
                # ä½¿ç”¨ ESM C SDK API
                protein = self.ESMProtein(sequence=seq)
                protein_tensor = self.esm_model.encode(protein)
                
                if hasattr(protein_tensor, 'error'):
                    raise RuntimeError(f"ESM C ç¼–ç å¤±è´¥: {protein_tensor.error}")
                
                # è·å–ç¬¬30å±‚çš„ embeddings
                # æ³¨æ„ï¼šith_hidden_layer å‚æ•°æŒ‡å®šå±‚ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼Œæ‰€ä»¥30å±‚æ˜¯ç´¢å¼•29æˆ–30ï¼‰
                # æ ¹æ® ESM C æ–‡æ¡£ï¼Œ-1 è¡¨ç¤ºæœ€åä¸€å±‚ï¼Œæ­£æ•´æ•°è¡¨ç¤ºç‰¹å®šå±‚
                logits_config = self.LogitsConfig(
                    sequence=True, 
                    return_embeddings=True,
                    ith_hidden_layer=self.esm_layer  # ä½¿ç”¨ç¬¬30å±‚
                )
                logits_output = self.esm_model.logits(protein_tensor, logits_config)
                
                # ESM C è¿”å›çš„ embeddings å¯èƒ½æ˜¯ numpy æ•°ç»„æˆ– torch å¼ é‡
                if hasattr(logits_output, 'embeddings'):
                    embeddings = logits_output.embeddings
                    # è½¬æ¢ä¸º torch å¼ é‡ï¼ˆå¦‚æœæ˜¯ numpy æ•°ç»„ï¼‰
                    if not isinstance(embeddings, torch.Tensor):
                        embeddings = torch.tensor(embeddings, device=self.device)
                    else:
                        embeddings = embeddings.to(self.device)
                    embeddings = embeddings.squeeze(0)
                else:
                    raise RuntimeError("ESM C logits_output æ²¡æœ‰ embeddings å±æ€§")
                
                batch_embeddings.append(embeddings)
            except Exception as e:
                raise RuntimeError(f"âŒ ESM C 600M å¤„ç†åºåˆ—å¤±è´¥: {e}")
        
        # å¯¹é½åºåˆ—é•¿åº¦ï¼ˆpaddingï¼‰
        if len(batch_embeddings) == 0:
            raise RuntimeError("æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½• embeddings")
        
        max_len = max(emb.shape[0] for emb in batch_embeddings)
        embed_dim = batch_embeddings[0].shape[1]
        batch_size = len(batch_embeddings)
        
        padded_embeddings = torch.zeros(batch_size, max_len, embed_dim, device=self.device)
        for i, emb in enumerate(batch_embeddings):
            seq_len = emb.shape[0]
            # ç¡®ä¿ emb åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if isinstance(emb, torch.Tensor):
                if emb.device != self.device:
                    emb = emb.to(self.device)
                padded_embeddings[i, :seq_len, :] = emb[:seq_len, :]
            else:
                emb_tensor = torch.tensor(emb[:seq_len], device=self.device)
                padded_embeddings[i, :seq_len, :] = emb_tensor
        
        return padded_embeddings
    
    @torch.no_grad()
    def generate_block_outputs(self, sequences: list, batch_size: int = 32, 
                                return_pooled: bool = False, max_sequence_length: int = None):
        """
        ä¸ºåºåˆ—åˆ—è¡¨ç”Ÿæˆ block è¾“å‡ºï¼ˆä¸ç»è¿‡ headsï¼‰ã€‚
        
        @param sequences: è›‹ç™½è´¨åºåˆ—åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²ï¼‰
        @param batch_size: æ¨ç†çš„æ‰¹æ¬¡å¤§å°
        @param return_pooled: å¦‚æœä¸º Trueï¼Œè¿”å›æ± åŒ–çš„ embeddingsï¼ˆå¹³å‡æ± åŒ–ï¼‰ã€‚
                            å¦‚æœä¸º Falseï¼Œè¿”å›åºåˆ—çº§åˆ«çš„ embeddingsï¼ˆæ‰€æœ‰ tokenï¼‰
        @param max_sequence_length: tokenization çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚
                                   å¦‚æœä¸º Noneï¼Œä½¿ç”¨å®ä¾‹çš„ max_sequence_length
        @returns: å¦‚æœ return_pooled=Trueï¼Œè¿”å›å½¢çŠ¶ä¸º (num_sequences, hidden_size) çš„å¼ é‡ï¼Œ
                 å¦‚æœ return_pooled=Falseï¼Œè¿”å›å½¢çŠ¶ä¸º (num_sequences, seq_len, hidden_size) çš„å¼ é‡
        """
        all_outputs = []
        
        # ä½¿ç”¨æä¾›çš„ max_sequence_length æˆ–å›é€€åˆ°å®ä¾‹é»˜è®¤å€¼
        max_seq_len = max_sequence_length if max_sequence_length is not None else self.max_sequence_length
        
        # åˆ†æ‰¹å¤„ç†
        for i in tqdm(range(0, len(sequences), batch_size), desc="ç”Ÿæˆ block è¾“å‡º"):
            batch_sequences = sequences[i:i + batch_size]
            
            # ä½¿ç”¨ ESM C 600M è®¡ç®— embeddings
            esm_embeddings = self._compute_esmc_embedding(batch_sequences)
            
            # å¯¹é½åºåˆ—é•¿åº¦ï¼ˆå¦‚æœéœ€è¦æˆªæ–­ï¼‰
            if max_seq_len is not None:
                # æˆªæ–­æˆ–å¡«å……åˆ° max_seq_len
                current_len = esm_embeddings.shape[1]
                if current_len > max_seq_len:
                    esm_embeddings = esm_embeddings[:, :max_seq_len, :]
                elif current_len < max_seq_len:
                    # å¡«å……åˆ° max_seq_len
                    batch_size_actual = esm_embeddings.shape[0]
                    embed_dim = esm_embeddings.shape[2]
                    padding = torch.zeros(batch_size_actual, max_seq_len - current_len, embed_dim, 
                                         device=self.device)
                    esm_embeddings = torch.cat([esm_embeddings, padding], dim=1)
            
            # é€šè¿‡ block å¤„ç† embeddingsï¼ˆåªè¿”å› block è¾“å‡ºï¼Œä¸ç»è¿‡ headsï¼‰
            block_outputs = self.model.block(esm_embeddings)  # (batch_size, seq_len, d_model)
            
            if return_pooled:
                # å¯¹åºåˆ—é•¿åº¦è¿›è¡Œå¹³å‡æ± åŒ–ï¼ˆæ’é™¤ paddingï¼‰
                # è®¡ç®—å®é™…åºåˆ—é•¿åº¦ï¼ˆéé›¶éƒ¨åˆ†ï¼‰
                # æ³¨æ„ï¼šESM C embeddings å¯èƒ½æ²¡æœ‰æ˜ç¡®çš„ padding tokenï¼Œæˆ‘ä»¬ä½¿ç”¨éé›¶è¡Œæ¥åˆ¤æ–­
                seq_lengths = []
                for j, seq in enumerate(batch_sequences):
                    seq_len = len(seq)
                    if max_seq_len is not None:
                        seq_len = min(seq_len, max_seq_len)
                    seq_lengths.append(seq_len)
                
                pooled_outputs = []
                for j, seq_len in enumerate(seq_lengths):
                    seq_output = block_outputs[j, :seq_len]  # (seq_len, d_model)
                    pooled = seq_output.mean(dim=0)  # (d_model,)
                    pooled_outputs.append(pooled)
                
                batch_pooled = torch.stack(pooled_outputs, dim=0)  # (batch_size, d_model)
                all_outputs.append(batch_pooled.cpu())
            else:
                # è¿”å›æ‰€æœ‰ token embeddingsï¼ˆå¯ç”¨äºé€ä½ç½®é¢„æµ‹ï¼‰
                all_outputs.append(block_outputs.cpu())
        
        # è¿æ¥æ‰€æœ‰æ‰¹æ¬¡
        outputs = torch.cat(all_outputs, dim=0)
        return outputs
    
    @torch.no_grad()
    def generate_per_position_block_outputs(self, sequences: list, batch_size: int = 32,
                                            max_sequence_length: int = None,
                                            use_sliding_window: bool = True,
                                            window_overlap: float = 0.5):
        """
        ä¸ºåºåˆ—ç”Ÿæˆé€ä½ç½®çš„ block è¾“å‡ºï¼ˆç”¨äºä½ç‚¹é¢„æµ‹ç­‰ä»»åŠ¡ï¼‰ã€‚
        å¯¹é•¿åºåˆ—ä½¿ç”¨æ»‘åŠ¨çª—å£ä»¥ä¿ç•™æ‰€æœ‰ä½ç½®ã€‚
        
        @param sequences: è›‹ç™½è´¨åºåˆ—åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²ï¼‰
        @param batch_size: æ¨ç†çš„æ‰¹æ¬¡å¤§å°ï¼ˆç”¨äºçª—å£å¤„ç†ï¼‰
        @param max_sequence_length: å•ä¸ªçª—å£çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚
                                   å¦‚æœä¸º Noneï¼Œä½¿ç”¨å®ä¾‹çš„ max_sequence_length
        @param use_sliding_window: å¦‚æœä¸º Trueï¼Œå¯¹é•¿äº max_sequence_length çš„åºåˆ—ä½¿ç”¨æ»‘åŠ¨çª—å£ã€‚
                                  å¦‚æœä¸º Falseï¼Œæˆªæ–­é•¿åºåˆ—ï¼ˆä¸æ¨èç”¨äºä½ç‚¹é¢„æµ‹ï¼‰
        @param window_overlap: çª—å£ä¹‹é—´çš„é‡å æ¯”ä¾‹ï¼ˆ0.0 åˆ° 1.0ï¼Œé»˜è®¤ 0.5 è¡¨ç¤º 50% é‡å ï¼‰ã€‚
                             æ›´é«˜çš„é‡å æä¾›æ›´å¥½çš„ä¸Šä¸‹æ–‡ï¼Œä½†éœ€è¦æ›´å¤šè®¡ç®—
        @returns: å¼ é‡åˆ—è¡¨ï¼Œæ¯ä¸ªå½¢çŠ¶ä¸º (seq_len, hidden_size)
        """
        all_outputs = []
        original_lengths = [len(seq) for seq in sequences]
        
        # ä½¿ç”¨æä¾›çš„ max_sequence_length æˆ–å›é€€åˆ°å®ä¾‹é»˜è®¤å€¼
        max_seq_len = max_sequence_length if max_sequence_length is not None else self.max_sequence_length
        
        # å¤„ç†æ¯ä¸ªåºåˆ—
        for seq_idx, sequence in enumerate(tqdm(sequences, desc="ç”Ÿæˆé€ä½ç½® block è¾“å‡º")):
            seq_len = len(sequence)
            
            # å¦‚æœåºåˆ—é€‚åˆä¸€ä¸ªçª—å£ï¼ˆseq_len <= max_seq_lenï¼‰æˆ–ç¦ç”¨æ»‘åŠ¨çª—å£
            if max_seq_len is None or seq_len <= max_seq_len or not use_sliding_window:
                # ä½œä¸ºå•ä¸ªçª—å£å¤„ç†
                esm_embeddings = self._compute_esmc_embedding([sequence])
                
                # å¯¹é½åˆ° max_seq_lenï¼ˆå¦‚æœéœ€è¦ï¼‰
                if max_seq_len is not None and esm_embeddings.shape[1] > max_seq_len:
                    esm_embeddings = esm_embeddings[:, :max_seq_len, :]
                elif max_seq_len is not None and esm_embeddings.shape[1] < max_seq_len:
                    # å¡«å……
                    current_len = esm_embeddings.shape[1]
                    embed_dim = esm_embeddings.shape[2]
                    padding = torch.zeros(1, max_seq_len - current_len, embed_dim, device=self.device)
                    esm_embeddings = torch.cat([esm_embeddings, padding], dim=1)
                
                # é€šè¿‡ block å¤„ç†ï¼ˆéœ€è¦æ·»åŠ  batch ç»´åº¦ï¼‰
                block_output = self.model.block(esm_embeddings)  # (1, seq_len, d_model)
                block_output = block_output[0]  # (seq_len, d_model)
                
                # ç¡®ä¿è¾“å‡ºé•¿åº¦åŒ¹é…åŸå§‹åºåˆ—é•¿åº¦
                output_len = block_output.shape[0]
                if output_len != seq_len:
                    if output_len < seq_len:
                        # å¡«å……ä»¥åŒ¹é…åºåˆ—é•¿åº¦
                        pad_size = seq_len - output_len
                        padding = torch.zeros(pad_size, block_output.shape[1], device=block_output.device)
                        block_output = torch.cat([block_output, padding], dim=0)
                    else:
                        # æˆªæ–­
                        block_output = block_output[:seq_len]
                
                all_outputs.append(block_output.cpu())
            else:
                # å¯¹é•¿åºåˆ—ä½¿ç”¨æ»‘åŠ¨çª—å£ï¼ˆseq_len > max_seq_lenï¼‰
                windows = self._create_sliding_windows(sequence, max_seq_len, window_overlap)
                
                # åˆ†æ‰¹å¤„ç†çª—å£
                window_outputs_list = []
                for i in range(0, len(windows), batch_size):
                    batch_windows = windows[i:i + batch_size]
                    batch_seqs = [w[0] for w in batch_windows]
                    
                    # è®¡ç®— ESM C embeddings
                    batch_esm_embeddings = self._compute_esmc_embedding(batch_seqs)
                    
                    # å¯¹é½åˆ° max_seq_len
                    if batch_esm_embeddings.shape[1] > max_seq_len:
                        batch_esm_embeddings = batch_esm_embeddings[:, :max_seq_len, :]
                    
                    # é€šè¿‡ block å¤„ç†æ¯ä¸ªçª—å£
                    batch_outputs = []
                    for j in range(batch_esm_embeddings.shape[0]):
                        window_emb = batch_esm_embeddings[j]
                        window_output = self.model.block(window_emb.unsqueeze(0))[0]  # (window_len, d_model)
                        batch_outputs.append(window_output)
                    
                    window_outputs_list.extend(batch_outputs)
                
                # åˆå¹¶çª—å£è¾“å‡º
                windows_data = [(out, start, end) for out, (_, start, end) in zip(window_outputs_list, windows)]
                merged_outputs = self._merge_window_outputs(windows_data, seq_len, self.hidden_size)
                # ç§»åŠ¨åˆ° CPUï¼ˆå› ä¸ºæœ€ç»ˆè¦è¿”å›ç»™ç”¨æˆ·ï¼‰
                all_outputs.append(merged_outputs.cpu())
        
        return all_outputs, original_lengths
    
    def _create_sliding_windows(self, sequence: str, window_size: int, overlap: float = 0.5):
        """
        ä¸ºé•¿åºåˆ—åˆ›å»ºæ»‘åŠ¨çª—å£ã€‚
        
        @param sequence: è¾“å…¥åºåˆ—å­—ç¬¦ä¸²
        @param window_size: æ¯ä¸ªçª—å£çš„å¤§å°
        @param overlap: çª—å£ä¹‹é—´çš„é‡å æ¯”ä¾‹ï¼ˆ0.0 åˆ° 1.0ï¼Œé»˜è®¤ 0.5 è¡¨ç¤º 50% é‡å ï¼‰
        @returns: (çª—å£åºåˆ—, start_idx, end_idx) å…ƒç»„åˆ—è¡¨
        """
        windows = []
        seq_len = len(sequence)
        step_size = max(1, int(window_size * (1 - overlap)))  # ç¡®ä¿ step_size >= 1
        
        start = 0
        while start < seq_len:
            end = min(start + window_size, seq_len)
            window_seq = sequence[start:end]
            windows.append((window_seq, start, end))
            
            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªçª—å£
            start += step_size
            
            # å¦‚æœè¿˜æ²¡æœ‰åˆ°è¾¾æœ«å°¾ï¼Œä½†ä¸‹ä¸€ä¸ªçª—å£ä¼šè¶…å‡ºåºåˆ—ï¼Œ
            # åˆ›å»ºä¸€ä¸ªä»¥åºåˆ—æœ«å°¾ç»“æŸçš„æœ€ç»ˆçª—å£
            if start < seq_len and start + window_size > seq_len:
                # åˆ›å»ºä¸€ä¸ªè¦†ç›–å‰©ä½™éƒ¨åˆ†çš„æœ€ç»ˆçª—å£
                final_start = max(0, seq_len - window_size)
                if final_start > start - step_size:  # ä»…å½“ä¸å‰ä¸€ä¸ªä¸åŒæ—¶æ·»åŠ 
                    final_window_seq = sequence[final_start:seq_len]
                    windows.append((final_window_seq, final_start, seq_len))
                break
        
        return windows
    
    def _merge_window_outputs(self, windows_data: list, full_length: int, hidden_size: int):
        """
        åˆå¹¶æ¥è‡ªå¤šä¸ªæ»‘åŠ¨çª—å£çš„è¾“å‡ºã€‚
        å¯¹äºé‡å åŒºåŸŸï¼Œå–è¾“å‡ºçš„å¹³å‡å€¼ã€‚
        
        @param windows_data: (è¾“å‡ºå¼ é‡, start_idx, end_idx) å…ƒç»„åˆ—è¡¨
        @param full_length: å®Œæ•´åºåˆ—é•¿åº¦
        @param hidden_size: éšè—ç»´åº¦å¤§å°
        @returns: åˆå¹¶çš„è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º (full_length, hidden_size)
        """
        # ç¡®å®šè®¾å¤‡ï¼ˆä»ç¬¬ä¸€ä¸ªçª—å£è¾“å‡ºè·å–ï¼‰
        if len(windows_data) > 0 and len(windows_data[0]) > 0:
            device = windows_data[0][0].device if isinstance(windows_data[0][0], torch.Tensor) else self.device
        else:
            device = self.device
        
        # åˆå§‹åŒ–è¾“å‡ºå¼ é‡å’Œè®¡æ•°å¼ é‡ç”¨äºå¹³å‡ï¼ˆåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼‰
        merged_outputs = torch.zeros(full_length, hidden_size, device=device)
        count_tensor = torch.zeros(full_length, device=device)
        
        for window_out, start_idx, end_idx in windows_data:
            # ç¡®ä¿ window_out åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if isinstance(window_out, torch.Tensor):
                if window_out.device != device:
                    window_out = window_out.to(device)
            else:
                window_out = torch.tensor(window_out, device=device)
            
            window_len = window_out.shape[0]
            actual_end = min(start_idx + window_len, full_length)
            actual_len = actual_end - start_idx
            
            # å°†è¾“å‡ºæ·»åŠ åˆ°åˆå¹¶å¼ é‡
            merged_outputs[start_idx:actual_end] += window_out[:actual_len]
            count_tensor[start_idx:actual_end] += 1
        
        # å¹³å‡é‡å åŒºåŸŸ
        # é¿å…é™¤ä»¥é›¶
        count_tensor = torch.clamp(count_tensor, min=1.0)
        merged_outputs = merged_outputs / count_tensor.unsqueeze(-1)
        
        return merged_outputs


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ä»é¢„è®­ç»ƒ LoRA æ¨¡å‹ç”Ÿæˆ block è¾“å‡º")
    parser.add_argument("--checkpoint", type=str, required=True, help="æ¨¡å‹ checkpoint è·¯å¾„")
    parser.add_argument("--sequences", type=str, nargs="+", help="è¾“å…¥åºåˆ—")
    parser.add_argument("--output", type=str, help="ä¿å­˜è¾“å‡ºçš„è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=32, help="æ¨ç†æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--return_pooled", action="store_true", help="è¿”å›æ± åŒ–çš„è¾“å‡º")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨ç†æ¨¡å‹
    inferencer = LoRAInference(args.checkpoint)
    
    # ç”Ÿæˆ block è¾“å‡º
    outputs = inferencer.generate_block_outputs(
        args.sequences,
        batch_size=args.batch_size,
        return_pooled=args.return_pooled
    )
    
    # å¦‚æœæä¾›äº†è¾“å‡ºè·¯å¾„ï¼Œä¿å­˜è¾“å‡º
    if args.output:
        torch.save(outputs, args.output)
        print(f"âœ… è¾“å‡ºå·²ä¿å­˜åˆ° {args.output}")
    else:
        print(f"ğŸ“Š ç”Ÿæˆçš„è¾“å‡ºå½¢çŠ¶: {outputs.shape}")

