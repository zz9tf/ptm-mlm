"""
PTM Dataset for memmap format.
Input: embeddings (pre-generated ESM embeddings) stored in memmap format
Output: original_sequence and ptm_sequence

ä½¿ç”¨ memmap æ ¼å¼çš„ä¼˜åŠ¿ï¼š
1. ä¸éœ€è¦ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
2. å¤šä¸ªè¿›ç¨‹å¯ä»¥å…±äº«åŒä¸€ä¸ª memmap æ–‡ä»¶ï¼ˆåªè¯»ï¼‰
3. æ•°æ®è®¿é—®æ›´å¿«ï¼ˆç›´æ¥å†…å­˜æ˜ å°„ï¼‰
4. æ”¯æŒå»¶è¿ŸåŠ è½½ï¼Œå¯åŠ¨é€Ÿåº¦å¿«
"""
import os
import json
import re
import torch
import numpy as np
import random
from typing import Any, Dict, List, Optional, Tuple
from torch.utils.data import Dataset as TorchDataset
from tqdm import tqdm


class PTMDatasetMemmap(TorchDataset):
    """
    PTM Dataset for memmap format.
    Input: embeddings (pre-generated ESM embeddings) stored in memmap format
    Output: original_sequence and ptm_sequence
    
    ä½¿ç”¨ memmap æ ¼å¼ï¼Œæ”¯æŒæŒ‰éœ€åŠ è½½ï¼Œå¤šä¸ªè¿›ç¨‹å…±äº«åŒä¸€æ–‡ä»¶ã€‚
    """

    def __init__(
        self,
        dataset_dir: str,
        device: Optional[torch.device] = None,
        seed: Optional[int] = None,
        val_size: Optional[int] = None,
        test_size: Optional[int] = None,
        preload_all: bool = False,
        use_functional_role: bool = False,
    ):
        """
        åˆå§‹åŒ– memmap æ ¼å¼çš„ datasetã€‚

        @param dataset_dir: åŒ…å« memmap æ–‡ä»¶çš„ç›®å½•ï¼ˆåŒ…å« meta_mapping.json, embeddings.dat ç­‰ï¼‰
        @param device: æ”¾ç½® embeddings çš„è®¾å¤‡
        @param seed: éšæœºç§å­ï¼Œç”¨äºçª—å£é€‰æ‹©å’Œæ•°æ®é›†åˆ†å‰²
        @param val_size: éªŒè¯é›†æ ·æœ¬æ•°
        @param test_size: æµ‹è¯•é›†æ ·æœ¬æ•°
        @param preload_all: æ˜¯å¦é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜ï¼ˆTrue=é¢„åŠ è½½æ¨¡å¼ï¼ŒFalse=memmapæŒ‰éœ€åŠ è½½æ¨¡å¼ï¼‰
        @param use_functional_role: æ˜¯å¦ä½¿ç”¨ functional role æ•°æ®ï¼ˆéœ€è¦é¢å¤–çš„ functional_role.dat å’Œ functional_role_position.dat æ–‡ä»¶ï¼‰
        """
        super().__init__()
        self.dataset_dir = dataset_dir
        self.device = device if device is not None else torch.device('cpu')
        self.rng = np.random.RandomState(seed)
        self.val_size = val_size if val_size is not None else 0
        self.test_size = test_size if test_size is not None else 0
        self.seed = seed
        self.preload_all = preload_all
        self.use_functional_role = use_functional_role
        
        # åŠ è½½å…ƒæ•°æ®
        meta_mapping_path = os.path.join(self.dataset_dir, "meta_mapping.json")
        if not os.path.exists(meta_mapping_path):
            raise FileNotFoundError(f"meta_mapping.json not found in {self.dataset_dir}")
        
        with open(meta_mapping_path, 'r') as f:
            self.meta_mapping = json.load(f)
        
        self.total_samples = self.meta_mapping['total_samples']
        self.embedding_dim = self.meta_mapping['embedding_dim']
        self.sequence_length = self.meta_mapping['sequence_length']
        self.idx_to_protein_id = self.meta_mapping['idx_to_protein_id']
        self.protein_id_to_idx = self.meta_mapping['protein_id_to_idx']
        
        # åŠ è½½æ•°æ®ï¼ˆæ ¹æ® preload_all æ ‡å¿—é€‰æ‹©æ¨¡å¼ï¼‰
        if self.preload_all:
            # é¢„åŠ è½½æ¨¡å¼ï¼šå°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°å†…å­˜
            print(f"ğŸ“¦ Preloading all data to memory ({self.total_samples:,} samples)...")
            
            # å®šä¹‰è¦åŠ è½½çš„æ–‡ä»¶ä¿¡æ¯
            files_to_load = [
                {
                    "name": "embeddings.dat",
                    "dtype": np.float16,
                    "shape": (self.total_samples, self.sequence_length, self.embedding_dim),
                    "attr_memmap": "embeddings_memmap",
                    "attr_data": "embeddings_data"
                },
                {
                    "name": "orig_tokens.dat",
                    "dtype": np.int32,
                    "shape": (self.total_samples, self.sequence_length),
                    "attr_memmap": "orig_tokens_memmap",
                    "attr_data": "orig_tokens_data"
                },
                {
                    "name": "ptm_tokens.dat",
                    "dtype": np.int32,
                    "shape": (self.total_samples, self.sequence_length),
                    "attr_memmap": "ptm_tokens_memmap",
                    "attr_data": "ptm_tokens_data"
                },
                {
                    "name": "range.dat",
                    "dtype": np.int32,
                    "shape": (self.total_samples, 3),
                    "attr_memmap": "range_memmap",
                    "attr_data": "range_data"
                },
                {
                    "name": "meta_id.dat",
                    "dtype": np.int64,
                    "shape": (self.total_samples,),
                    "attr_memmap": "meta_id_memmap",
                    "attr_data": "meta_id_data"
                }
            ]

            # å¦‚æœä½¿ç”¨functional roleï¼Œæ·»åŠ ç›¸å…³æ–‡ä»¶
            if self.use_functional_role:
                files_to_load.extend([
                    {
                        "name": "functional_role.dat",
                        "dtype": np.float32,
                        "shape": (self.total_samples, self.sequence_length),
                        "attr_memmap": "functional_role_memmap",
                        "attr_data": "functional_role_data"
                    },
                    {
                        "name": "functional_role_position.dat",
                        "dtype": np.int32,
                        "shape": (self.total_samples, self.sequence_length),
                        "attr_memmap": "functional_role_position_memmap",
                        "attr_data": "functional_role_position_data"
                    }
                ])
            
            # è®¡ç®—æ¯ä¸ªæ–‡ä»¶çš„å¤§å°å¹¶åˆ›å»ºè¿›åº¦æ¡
            total_size_bytes = 0
            file_sizes = []
            for file_info in files_to_load:
                file_size = np.prod(file_info["shape"]) * np.dtype(file_info["dtype"]).itemsize
                file_sizes.append(file_size)
                total_size_bytes += file_size
            
            total_size_gb = total_size_bytes / (1024**3)

            # åˆ›å»ºæ€»ä½“è¿›åº¦æ¡ï¼ˆä»¥ GB ä¸ºå•ä½ï¼‰
            pbar = tqdm(
                total=total_size_gb,
                desc="Preloading data",
                unit="GB",
                unit_scale=False,
                bar_format="{l_bar}{bar}| {n:.2f}/{total:.2f} GB"
            )
            
            # é€ä¸ªåŠ è½½æ–‡ä»¶
            for idx, file_info in enumerate(files_to_load):
                file_path = os.path.join(self.dataset_dir, file_info["name"])
                file_size = file_sizes[idx]
                file_size_gb = file_size / (1024**3)

                # æ›´æ–°è¿›åº¦æ¡æè¿°
                pbar.set_description(f"Loading {file_info['name']} ({file_size_gb:.2f} GB)")

                # åˆ›å»º memmap
                memmap_obj = np.memmap(
                    file_path,
                    dtype=file_info["dtype"],
                    mode='r',
                    shape=file_info["shape"]
                )
                setattr(self, file_info["attr_memmap"], memmap_obj)

                # åŠ è½½åˆ°å†…å­˜ï¼ˆè¿™ä¼šè§¦å‘å®é™…çš„æ•°æ®è¯»å–ï¼‰
                import time
                start_time = time.time()

                if file_info["name"] == "embeddings.dat":
                    # ğŸ¯ embeddings.dat ä½¿ç”¨ä¼˜åŒ–å—æ‹·è´ + ä½é¢‘æ›´æ–°ç­–ç•¥
                    data_array = np.empty(file_info["shape"], dtype=file_info["dtype"])

                    src = memmap_obj.reshape(-1)
                    dst = data_array.reshape(-1)

                    elem_size = np.dtype(file_info["dtype"]).itemsize
                    total_elems = src.size

                    # æ¯æ¬¡æ‹·è´ 1GBï¼ˆå¯è°ƒåˆ° 512MB/2GBï¼‰
                    chunk_bytes = 1024 * 1024**2
                    chunk_elems = max(1, chunk_bytes // elem_size)

                    # tqdm ä½é¢‘åˆ·æ–°ï¼šç´¯è®¡åˆ° ~1GB å† update ä¸€æ¬¡
                    update_every_gb = 1.0
                    accum_gb = 0.0

                    for i in range(0, total_elems, chunk_elems):
                        j = min(i + chunk_elems, total_elems)
                        dst[i:j] = src[i:j]

                        accum_gb += (j - i) * elem_size / (1024**3)
                        if accum_gb >= update_every_gb:
                            pbar.update(accum_gb)
                            accum_gb = 0.0

                    # æ”¶å°¾
                    if accum_gb > 0:
                        pbar.update(accum_gb)
                else:
                    # å…¶ä»–æ–‡ä»¶ä½¿ç”¨å¿«é€ŸåŠ è½½
                    data_array = np.array(memmap_obj)

                elapsed_time = time.time() - start_time
                load_speed_gbs = file_size_gb / elapsed_time if elapsed_time > 0 else 0

                setattr(self, file_info["attr_data"], data_array)

                # æ›´æ–°è¿›åº¦æ¡ï¼ˆæ›´æ–° GB æ•°ï¼‰- embeddings.dat å·²åœ¨å—æ‹·è´æ—¶æ›´æ–°ï¼Œè¿™é‡Œè·³è¿‡
                if file_info["name"] != "embeddings.dat":
                    pbar.update(file_size_gb)

                # åœ¨è¿›åº¦æ¡åæ˜¾ç¤ºæ–‡ä»¶åŠ è½½ä¿¡æ¯
                pbar.write(f"  âœ“ {file_info['name']}: {file_size_gb:.2f} GB loaded ({load_speed_gbs:.2f} GB/s)")

                # ç«‹å³åˆ é™¤ memmap å¯¹è±¡ä»¥èŠ‚çœå†…å­˜
                del memmap_obj

            pbar.close()
            
            # ä¼°ç®—å†…å­˜ä½¿ç”¨é‡
            embeddings_size_gb = self.embeddings_data.nbytes / (1024**3)
            tokens_size_gb = (self.orig_tokens_data.nbytes + self.ptm_tokens_data.nbytes) / (1024**3)
            range_size_gb = self.range_data.nbytes / (1024**3)
            meta_size_gb = self.meta_id_data.nbytes / (1024**3)
            total_size_gb = embeddings_size_gb + tokens_size_gb + range_size_gb + meta_size_gb
            print(f"âœ… Preloaded all data to memory: {total_size_gb:.2f} GB "
                  f"(embeddings: {embeddings_size_gb:.2f} GB, tokens: {tokens_size_gb:.2f} GB, "
                  f"range: {range_size_gb:.2f} GB, meta: {meta_size_gb:.2f} GB)")
        else:
            # Memmap æ¨¡å¼ï¼šæŒ‰éœ€åŠ è½½ï¼ˆå¤šä¸ªè¿›ç¨‹å¯ä»¥å…±äº«ï¼‰
            self.embeddings_memmap = np.memmap(
                os.path.join(self.dataset_dir, "embeddings.dat"),
                dtype=np.float16,
                mode='r',
                shape=(self.total_samples, self.sequence_length, self.embedding_dim)
            )
            self.orig_tokens_memmap = np.memmap(
                os.path.join(self.dataset_dir, "orig_tokens.dat"),
                dtype=np.int32,
                mode='r',
                shape=(self.total_samples, self.sequence_length)
            )
            self.ptm_tokens_memmap = np.memmap(
                os.path.join(self.dataset_dir, "ptm_tokens.dat"),
                dtype=np.int32,
                mode='r',
                shape=(self.total_samples, self.sequence_length)
            )
            self.range_memmap = np.memmap(
                os.path.join(self.dataset_dir, "range.dat"),
                dtype=np.int32,
                mode='r',
                shape=(self.total_samples, 3)  # [start, end, length]
            )
            self.meta_id_memmap = np.memmap(
                os.path.join(self.dataset_dir, "meta_id.dat"),
                dtype=np.int64,
                mode='r',
                shape=(self.total_samples,)
            )

            # å¦‚æœä½¿ç”¨functional roleï¼ŒåŠ è½½ç›¸å…³memmapæ–‡ä»¶
            if self.use_functional_role:
                self.functional_role_memmap = np.memmap(
                    os.path.join(self.dataset_dir, "functional_role.dat"),
                    dtype=np.float32,
                    mode='r',
                    shape=(self.total_samples, self.sequence_length)
                )
                self.functional_role_position_memmap = np.memmap(
                    os.path.join(self.dataset_dir, "functional_role_position.dat"),
                    dtype=np.int32,
                    mode='r',
                    shape=(self.total_samples, self.sequence_length)
                )
        
        # åˆå§‹åŒ– samples_by_split
        self.samples_by_split = {'train': [], 'val': [], 'test': []}

        # æ„å»ºæ ·æœ¬åˆ—è¡¨ï¼ˆæ ¹æ® PTM é˜ˆå€¼è¿‡æ»¤ï¼‰
        self._build_samples()

        # åˆ†å‰²æ•°æ®é›†
        if self.val_size > 0 or self.test_size > 0:
            self._split_samples()
        else:
            # å¦‚æœæ²¡æœ‰åˆ†å‰²ï¼Œæ‰€æœ‰æ ·æœ¬éƒ½åœ¨ train ä¸­
            for sample in self._all_samples:
                sample['split'] = 'train'
                self.samples_by_split['train'].append(sample)
            if hasattr(self, '_all_samples'):
                delattr(self, '_all_samples')

        # æ‰å¹³åŒ–ç´¢å¼•ä»¥ä¼˜åŒ– __len__ å’Œ __getitem__ æ€§èƒ½
        self._build_flat_index()

    def _build_samples(self):
        """
        æ„å»ºæ ·æœ¬åˆ—è¡¨ã€‚
        
        æŒ‰ä½ çš„éœ€æ±‚ï¼šä¸åšä»»ä½• PTM ç­›é€‰/éšæœºé‡‡æ ·ï¼Œåªæ˜¯ã€ŒæŒ‰ç´¢å¼•é¡ºåºã€æŠŠæ‰€æœ‰ sample_idx éƒ½çº³å…¥åˆ—è¡¨ï¼Œ
        æ–¹ä¾¿åšçº¯ load/å¸¦å®½æµ‹è¯•ã€‚
        """
        all_samples = []
        
        print(f"ğŸš€ Building sample list (no PTM filtering) from {self.total_samples:,} samples (memmap format)...")
        
        # ç›´æ¥ä¿ç•™æ‰€æœ‰æ ·æœ¬ç´¢å¼•
        for sample_idx in range(self.total_samples):
            all_samples.append({
                "sample_idx": sample_idx,
                "split": None,
            })
        
        self._all_samples = all_samples
        print(f"âœ… Built sample list: {len(all_samples):,} samples (from {self.total_samples:,} total)")
    
    def _split_samples(self):
        """
        å°†æ ·æœ¬åˆ†å‰²ä¸º train/val/testã€‚
        """
        all_samples = getattr(self, '_all_samples', [])
        total = len(all_samples)
        
        if self.val_size + self.test_size > total:
            raise ValueError(
                f"val_size + test_size exceeds dataset size ({self.val_size + self.test_size} > {total})"
            )
        
        # åˆ›å»ºç´¢å¼•å¹¶æ‰“ä¹±
        indices = list(range(total))
        split_rng = random.Random(self.seed)
        split_rng.shuffle(indices)
        
        # è®¡ç®—åˆ†å‰²è¾¹ç•Œ
        test_start = total - self.test_size
        val_start = test_start - self.val_size
        
        train_idx = indices[:val_start] if val_start > 0 else []
        val_idx = indices[val_start:test_start] if self.val_size > 0 else []
        test_idx = indices[test_start:] if self.test_size > 0 else []
        
        # åˆ†é…æ ·æœ¬åˆ°å„ä¸ª splitï¼ˆç¡®ä¿å·²åˆå§‹åŒ–ï¼‰
        if not hasattr(self, 'samples_by_split'):
            self.samples_by_split = {'train': [], 'val': [], 'test': []}
        
        for idx in train_idx:
            sample = all_samples[idx].copy()
            sample['split'] = 'train'
            self.samples_by_split['train'].append(sample)
        
        for idx in val_idx:
            sample = all_samples[idx].copy()
            sample['split'] = 'val'
            self.samples_by_split['val'].append(sample)
        
        for idx in test_idx:
            sample = all_samples[idx].copy()
            sample['split'] = 'test'
            self.samples_by_split['test'].append(sample)
        
        print(f"ğŸ“Š Dataset split (seed={self.seed}): "
              f"Train: {len(self.samples_by_split['train'])}, "
              f"Val: {len(self.samples_by_split['val'])}, "
              f"Test: {len(self.samples_by_split['test'])}")
        
        # æ¸…ç†ä¸´æ—¶å­˜å‚¨
        if hasattr(self, '_all_samples'):
            delattr(self, '_all_samples')

    def _build_flat_index(self):
        """æ„å»ºæ‰å¹³åŒ–ç´¢å¼•ä»¥ä¼˜åŒ–æ€§èƒ½"""
        # å¤„ç†å®Œæ•´æ•°æ®é›†ï¼ˆåŒ…å«æ‰€æœ‰splitsï¼‰å’Œå•ä¸ªsplitæ•°æ®é›†çš„æƒ…å†µ
        if len(self.samples_by_split) == 3 and all(k in self.samples_by_split for k in ['train', 'val', 'test']):
            # å®Œæ•´æ•°æ®é›†ï¼šæ‹¼æ¥æ‰€æœ‰splits
            self.flat_samples = (
                self.samples_by_split["train"] +
                self.samples_by_split["val"] +
                self.samples_by_split["test"]
            )
        else:
            # å•ä¸ªsplitæ•°æ®é›†ï¼šç›´æ¥ä½¿ç”¨è¯¥splitçš„æ ·æœ¬
            split_name = list(self.samples_by_split.keys())[0]
            self.flat_samples = self.samples_by_split[split_name]
        self.flat_len = len(self.flat_samples)

    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†å¤§å°ï¼ˆé¢„è®¡ç®—ï¼Œé¿å…æ¯æ¬¡æ±‚å’Œï¼‰"""
        return self.flat_len
    
    def get_split_samples(self, split_name: str) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®š split çš„æ ·æœ¬åˆ—è¡¨ã€‚
        
        @param split_name: Split åç§° ('train', 'val', æˆ– 'test')
        @return: æŒ‡å®š split çš„æ ·æœ¬åˆ—è¡¨
        """
        return self.samples_by_split.get(split_name, [])
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """
        è·å–å•ä¸ªæ ·æœ¬ï¼ˆä½¿ç”¨æ‰å¹³åŒ–ç´¢å¼•ä»¥ä¼˜åŒ–æ€§èƒ½ï¼‰ã€‚
        
        @param idx: æ ·æœ¬ç´¢å¼•ï¼ˆè·¨æ‰€æœ‰ splitsï¼‰
        @return: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
            - 'embeddings': torch.Tensor, shape (max_seq_len, embed_dim) float16 CPU
            - 'orig_ids': np.ndarray[int32]ï¼ŒåŸå§‹ token ids
            - 'ptm_ids': np.ndarray[int32]ï¼ŒPTM token ids
            - 'seq_length': int (å®é™…é•¿åº¦ï¼Œç”¨äº padding mask)
            - 'range': Tuple[int, int], (start, end) èŒƒå›´
            - 'sample_idx': int, åŸå§‹æ ·æœ¬ç´¢å¼•
            - 'protein_idx': int, è›‹ç™½è´¨ç´¢å¼•ï¼ˆé¿å…å­—ç¬¦ä¸²æŸ¥æ‰¾ï¼‰
        """
        if idx >= self.flat_len:
            raise IndexError(f"Index {idx} out of range (total samples: {self.flat_len})")

        sample = self.flat_samples[idx]
        sample_idx = sample['sample_idx']
        
        # æ ¹æ®é¢„åŠ è½½æ¨¡å¼é€‰æ‹©æ•°æ®æº
        if self.preload_all:
            # ä»å†…å­˜æ•°ç»„è¯»å–æ•°æ®
            embedding = self.embeddings_data[sample_idx]  # (512, 1152) float16
            orig_tokens = self.orig_tokens_data[sample_idx]  # (512,) int32
            ptm_tokens = self.ptm_tokens_data[sample_idx]  # (512,) int32
            range_data = self.range_data[sample_idx]  # (3,) int32 [start, end, length]
            protein_idx = int(self.meta_id_data[sample_idx])

            # å¦‚æœä½¿ç”¨functional roleï¼Œè¯»å–ç›¸å…³æ•°æ®
            if self.use_functional_role:
                functional_role = self.functional_role_data[sample_idx]  # (512,) float32
                functional_role_position = self.functional_role_position_data[sample_idx]  # (512,) int32
        else:
            # ä» memmap ä¸­è¯»å–æ•°æ®ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰
            embedding = self.embeddings_memmap[sample_idx]  # (512, 1152) float16
            orig_tokens = self.orig_tokens_memmap[sample_idx]  # (512,) int32
            ptm_tokens = self.ptm_tokens_memmap[sample_idx]  # (512,) int32
            range_data = self.range_memmap[sample_idx]  # (3,) int32 [start, end, length]
            protein_idx = int(self.meta_id_memmap[sample_idx])

            # å¦‚æœä½¿ç”¨functional roleï¼Œè¯»å–ç›¸å…³æ•°æ®
            if self.use_functional_role:
                functional_role = self.functional_role_memmap[sample_idx]  # (512,) float32
                functional_role_position = self.functional_role_position_memmap[sample_idx]  # (512,) int32
        
        # è·å–è›‹ç™½è´¨ ID
        protein_id = self.idx_to_protein_id[protein_idx]
        
        # è·å–å®é™…åºåˆ—é•¿åº¦
        seq_length = int(range_data[2])  # length
        range_tuple = (int(range_data[0]), int(range_data[1]))  # (start, end)

        # ä¿æŒ float16 CPU tensorï¼Œé¿å…ä¸å¿…è¦çš„è½¬æ¢å’Œä¼ è¾“
        embedding_tensor = torch.from_numpy(embedding)  # ä»æ˜¯ float16 CPU tensor

        result = {
            "embeddings": embedding_tensor,      # (max_seq_len, embed_dim) float16 CPU
            "orig_ids": orig_tokens,             # np.ndarray[int32]ï¼ŒåŸå§‹ ids
            "ptm_ids": ptm_tokens,               # np.ndarray[int32]ï¼ŒPTM ids
            "seq_length": seq_length,
            "range": range_tuple,
            "sample_idx": sample_idx,
            "protein_idx": protein_idx,          # ç›´æ¥ç”¨ intï¼Œé¿å…å­—ç¬¦ä¸²æŸ¥æ‰¾
        }

        # å¦‚æœä½¿ç”¨functional roleï¼Œæ·»åŠ ç›¸å…³æ•°æ®
        if self.use_functional_role:
            result["functional_role"] = functional_role  # np.ndarray[float32]ï¼Œfunctional role å€¼
            result["functional_role_position"] = functional_role_position  # np.ndarray[int32]ï¼Œfunctional role ä½ç½®

        return result
    
    def get_split_datasets(self) -> Dict[str, Optional["PTMDatasetMemmap"]]:
        """
        è·å–å„ä¸ª split çš„æ•°æ®é›†ã€‚
        
        @return: åŒ…å« train/val/test PTMDatasetMemmap çš„å­—å…¸ï¼›val/test å¯èƒ½ä¸º None
        """
        splits = {}
        
        for split_name in ['train', 'val', 'test']:
            split_samples = self.samples_by_split[split_name]
            if not split_samples:
                splits[split_name] = None
                continue
            
            # åˆ›å»ºæ–°çš„ dataset å®ä¾‹ï¼ˆå…±äº«æ•°æ®ï¼‰
            dataset = PTMDatasetMemmap.__new__(PTMDatasetMemmap)
            dataset.dataset_dir = self.dataset_dir
            dataset.device = self.device
            dataset.rng = np.random.RandomState(self.rng.randint(0, 2**31) if self.rng is not None else None)
            dataset.preload_all = self.preload_all
            
            # å…±äº«æ•°æ®ï¼ˆmemmap æˆ–é¢„åŠ è½½çš„å†…å­˜æ•°ç»„ï¼‰
            if self.preload_all:
                # å…±äº«é¢„åŠ è½½çš„å†…å­˜æ•°ç»„
                dataset.embeddings_data = self.embeddings_data
                dataset.orig_tokens_data = self.orig_tokens_data
                dataset.ptm_tokens_data = self.ptm_tokens_data
                dataset.range_data = self.range_data
                dataset.meta_id_data = self.meta_id_data

                # å¦‚æœä½¿ç”¨functional roleï¼Œå…±äº«ç›¸å…³æ•°æ®
                if self.use_functional_role:
                    dataset.functional_role_data = self.functional_role_data
                    dataset.functional_role_position_data = self.functional_role_position_data
            else:
                # å…±äº« memmap æ–‡ä»¶ï¼ˆåªè¯»ï¼Œå¯ä»¥å®‰å…¨å…±äº«ï¼‰
                dataset.embeddings_memmap = self.embeddings_memmap
                dataset.orig_tokens_memmap = self.orig_tokens_memmap
                dataset.ptm_tokens_memmap = self.ptm_tokens_memmap
                dataset.range_memmap = self.range_memmap
                dataset.meta_id_memmap = self.meta_id_memmap

                # å¦‚æœä½¿ç”¨functional roleï¼Œå…±äº«ç›¸å…³memmap
                if self.use_functional_role:
                    dataset.functional_role_memmap = self.functional_role_memmap
                    dataset.functional_role_position_memmap = self.functional_role_position_memmap
            
            dataset.meta_mapping = self.meta_mapping
            dataset.total_samples = self.total_samples
            dataset.embedding_dim = self.embedding_dim
            dataset.sequence_length = self.sequence_length
            dataset.idx_to_protein_id = self.idx_to_protein_id
            dataset.protein_id_to_idx = self.protein_id_to_idx
            
            dataset.samples_by_split = {split_name: split_samples}
            dataset.seed = self.seed
            dataset.val_size = self.val_size
            dataset.test_size = self.test_size

            # ç¡®ä¿ split datasets ä¹Ÿæœ‰æ‰å¹³åŒ–ç´¢å¼•
            dataset._build_flat_index()
            
            splits[split_name] = dataset
        
        return splits
    
    def get_split_mapping(self) -> Dict[str, str]:
        """
        è·å– unique_id åˆ° split çš„æ˜ å°„ã€‚
        
        @return: æ˜ å°„ unique_id åˆ° split åç§°çš„å­—å…¸
        """
        split_mapping = {}
        for split_name in ['train', 'val', 'test']:
            for sample in self.samples_by_split[split_name]:
                sample_idx = sample['sample_idx']
                if self.preload_all:
                    protein_idx = int(self.meta_id_data[sample_idx])
                else:
                    protein_idx = int(self.meta_id_memmap[sample_idx])
                protein_id = self.idx_to_protein_id[protein_idx]
                split_mapping[protein_id] = split_name
        return split_mapping


def get_ptm_dataset_memmap(
    dataset_dir: str,
    device: Optional[torch.device] = None,
    seed: Optional[int] = None,
    val_size: Optional[int] = None,
    test_size: Optional[int] = None,
    preload_all: bool = False,
    use_functional_role: bool = False,
) -> Dict[str, Optional[PTMDatasetMemmap]]:
    """
    ä» memmap æ ¼å¼åŠ è½½ PTM æ•°æ®é›†å¹¶åˆ†å‰²ä¸º train/val/testã€‚

    @param dataset_dir: åŒ…å« memmap æ–‡ä»¶çš„ç›®å½•
    @param device: æ”¾ç½® embeddings çš„è®¾å¤‡
    @param seed: éšæœºç§å­ï¼Œç”¨äºçª—å£é€‰æ‹©å’Œæ•°æ®é›†åˆ†å‰²
    @param val_size: éªŒè¯é›†æ ·æœ¬æ•°
    @param test_size: æµ‹è¯•é›†æ ·æœ¬æ•°
    @param preload_all: æ˜¯å¦é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜ï¼ˆTrue=é¢„åŠ è½½æ¨¡å¼ï¼ŒFalse=memmapæŒ‰éœ€åŠ è½½æ¨¡å¼ï¼‰
    @param use_functional_role: æ˜¯å¦ä½¿ç”¨ functional role æ•°æ®
    @return: åŒ…å« train/val/test PTMDatasetMemmap splits å’Œ split_mapping çš„å­—å…¸
    """
    dataset = PTMDatasetMemmap(
        dataset_dir=dataset_dir,
        device=device,
        seed=seed,
        val_size=val_size,
        test_size=test_size,
        preload_all=preload_all,
        use_functional_role=use_functional_role,
    )
    
    # è·å– split datasets
    splits = dataset.get_split_datasets()
    
    # è·å– split mapping å¹¶æ·»åŠ åˆ°è¿”å›å­—å…¸
    split_mapping = dataset.get_split_mapping()
    splits["split_mapping"] = split_mapping
    
    return splits

